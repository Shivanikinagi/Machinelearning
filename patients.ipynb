{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EEG Analysis Pipeline ===\n",
      "\n",
      "\n",
      "!!! Pipeline Failed !!!\n",
      "Error: 'EEGPipeline' object has no attribute 'load_dataset'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shivani\\AppData\\Local\\Temp\\ipykernel_7468\\357773262.py\", line 236, in run_pipeline\n",
      "    healthy_data, healthy_labels, h_sfreq = self.load_dataset(\n",
      "                                            ^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'EEGPipeline' object has no attribute 'load_dataset'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import mne\n",
    "import gc\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from scipy.signal import welch\n",
    "import pywt\n",
    "import pandas as pd\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Configure plotting\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "class EEGPipeline:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.scaler = None\n",
    "        self.label_encoder = None\n",
    "        self.common_channels = None\n",
    "        self.feature_names = [\n",
    "            'Delta Power', 'Theta Power', 'Alpha Power', 'Beta Power', 'Gamma Power',\n",
    "            'Wavelet Mean 1', 'Wavelet Mean 2', 'Wavelet Mean 3', 'Wavelet Mean 4', 'Wavelet Mean 5',\n",
    "            'Mean', 'Std Dev', 'Median', 'Skewness', 'Kurtosis',\n",
    "            'Hjorth Mobility', 'Hjorth Complexity', 'Spectral Entropy', 'Zero-Crossings', 'Peak-to-Peak'\n",
    "        ]\n",
    "        self.band_names = ['Delta', 'Theta', 'Alpha', 'Beta', 'Gamma']\n",
    "        self.channel_mapping = {}\n",
    "        self.expected_features_per_channel = len(self.feature_names)\n",
    "        self._error_messages = []  # Track errors during pipeline execution\n",
    "\n",
    "    def set_channel_mapping(self, mapping_dict):\n",
    "        \"\"\"Set manual channel name mapping between different naming conventions\"\"\"\n",
    "        self.channel_mapping = mapping_dict\n",
    "\n",
    "    def normalize_channel_name(self, channel_name):\n",
    "        \"\"\"Advanced channel name normalization with manual mapping support\"\"\"\n",
    "        if isinstance(channel_name, (list, np.ndarray)):\n",
    "            channel_name = channel_name[0]\n",
    "        channel_name = str(channel_name).strip().upper()\n",
    "        if channel_name in self.channel_mapping:\n",
    "            return self.channel_mapping[channel_name]\n",
    "        channel_name = re.sub(r'[^A-Z0-9]', '', channel_name)\n",
    "        channel_name = re.sub(r'^CH', '', channel_name)\n",
    "        channel_name = re.sub(r'^EEG', '', channel_name)\n",
    "        channel_name = channel_name.lstrip('0')\n",
    "        variations = {\n",
    "            'FP1': 'Fp1', 'FP2': 'Fp2',\n",
    "            'T3': 'T7', 'T4': 'T8',\n",
    "            'T5': 'P7', 'T6': 'P8'\n",
    "        }\n",
    "        return variations.get(channel_name, channel_name)\n",
    "\n",
    "    def get_channel_names_from_mat(self, mat_path):\n",
    "        \"\"\"Robust MAT file channel extraction supporting multiple formats\"\"\"\n",
    "        try:\n",
    "            mat_data = loadmat(mat_path)\n",
    "            channels = []\n",
    "            if 'data' in mat_data:\n",
    "                data_struct = mat_data['data'][0][0]\n",
    "                if 'channels' in data_struct.dtype.names:\n",
    "                    channels = [str(ch[0]) for ch in data_struct['channels'][0]]\n",
    "                elif 'chanlocs' in data_struct.dtype.names:\n",
    "                    chanlocs = data_struct['chanlocs'][0]\n",
    "                    channels = [str(chan['labels'][0]) for chan in chanlocs]\n",
    "            elif 'EEG' in mat_data:\n",
    "                eeg_struct = mat_data['EEG'][0][0]\n",
    "                if 'chanlocs' in eeg_struct.dtype.names:\n",
    "                    chanlocs = eeg_struct['chanlocs'][0]\n",
    "                    channels = [str(chan['labels'][0]) for chan in chanlocs]\n",
    "                elif 'chaninfo' in eeg_struct.dtype.names:\n",
    "                    chaninfo = eeg_struct['chaninfo'][0][0]\n",
    "                    if 'labels' in chaninfo.dtype.names:\n",
    "                        channels = [str(ch[0]) for ch in chaninfo['labels'][0]]\n",
    "            elif 'X' in mat_data and 'ch_names' in mat_data:\n",
    "                channels = [str(ch[0]) for ch in mat_data['ch_names'][0]]\n",
    "            return [self.normalize_channel_name(ch) for ch in channels if ch and str(ch).strip()]\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading {mat_path}: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return []\n",
    "\n",
    "    def preprocess_data(self, data, labels, sfreq=250, common_channels=None):\n",
    "        \"\"\"Enhanced preprocessing with additional features\"\"\"\n",
    "        try:\n",
    "            if data is None or len(data) == 0:\n",
    "                raise ValueError(\"Empty data array\")\n",
    "            n_channels = data.shape[1]\n",
    "            ch_names = common_channels[:n_channels] if common_channels else [f'ch{i}' for i in range(n_channels)]\n",
    "            info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')\n",
    "            raw = mne.io.RawArray(data.T, info)\n",
    "            nyquist = sfreq / 2\n",
    "            raw.filter(0.5, min(40, nyquist-1), fir_design='firwin', phase='zero-double')\n",
    "            notch_freqs = [50, 60]\n",
    "            notch_freqs = [f for f in notch_freqs if f < nyquist]\n",
    "            if notch_freqs:\n",
    "                raw.notch_filter(notch_freqs)\n",
    "            events = mne.make_fixed_length_events(raw, duration=1.0)\n",
    "            epochs = mne.Epochs(raw, events, tmin=0, tmax=1.0, baseline=None, preload=True)\n",
    "            epochs_data = epochs.get_data()\n",
    "\n",
    "            def extract_features(epoch_data):\n",
    "                features = []\n",
    "                for epoch in epoch_data:\n",
    "                    epoch_features = []\n",
    "                    for channel in epoch:\n",
    "                        # Frequency features\n",
    "                        freqs, psd = welch(channel, fs=sfreq, nperseg=min(256, len(channel)))\n",
    "                        bands = {'delta': (0.5, 4), 'theta': (4, 8), 'alpha': (8, 12),\n",
    "                                 'beta': (12, 30), 'gamma': (30, min(50, nyquist-1))}\n",
    "                        band_powers = [np.sum(psd[(freqs >= low) & (freqs <= high)]) \n",
    "                                     for low, high in bands.values()]\n",
    "                        # Wavelet features\n",
    "                        coeffs = pywt.wavedec(channel, 'db4', level=4)\n",
    "                        wavelet_features = [np.mean(c) for c in coeffs[:5]]\n",
    "                        if len(wavelet_features) < 5:\n",
    "                            wavelet_features += [0.0] * (5 - len(wavelet_features))\n",
    "                        # Statistical features\n",
    "                        stats = [\n",
    "                            np.mean(channel), np.std(channel), np.median(channel),\n",
    "                            pd.Series(channel).skew(), pd.Series(channel).kurtosis()\n",
    "                        ]\n",
    "                        # Hjorth parameters\n",
    "                        mobility, complexity = self.hjorth_parameters(channel)\n",
    "                        # Spectral entropy\n",
    "                        spectral_entropy = -np.sum(psd * np.log(psd + 1e-10))\n",
    "                        # Zero-crossing rate\n",
    "                        zero_crossings = np.where(np.diff(np.sign(channel)))[0].size\n",
    "                        # Peak-to-peak amplitude\n",
    "                        peak_to_peak = np.max(channel) - np.min(channel)\n",
    "                        epoch_features.extend(\n",
    "                            band_powers + wavelet_features + stats + \n",
    "                            [mobility, complexity, spectral_entropy, zero_crossings, peak_to_peak]\n",
    "                        )\n",
    "                    features.append(epoch_features)\n",
    "                return np.array(features)\n",
    "\n",
    "            X = extract_features(epochs_data)\n",
    "            y = labels[:len(X)]\n",
    "            expected_features = n_channels * self.expected_features_per_channel\n",
    "            if X.shape[1] != expected_features:\n",
    "                if X.shape[1] < expected_features:\n",
    "                    pad_width = ((0, 0), (0, expected_features - X.shape[1]))\n",
    "                    X = np.pad(X, pad_width, mode='constant')\n",
    "                else:\n",
    "                    X = X[:, :expected_features]\n",
    "            return X, y\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error during preprocessing (sfreq={sfreq}): {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "    def hjorth_parameters(self, signal):\n",
    "        \"\"\"Calculate Hjorth mobility and complexity parameters\"\"\"\n",
    "        first_deriv = np.diff(signal)\n",
    "        second_deriv = np.diff(signal, 2)\n",
    "        var_zero = np.var(signal)\n",
    "        var_d1 = np.var(first_deriv)\n",
    "        var_d2 = np.var(second_deriv)\n",
    "        mobility = np.sqrt(var_d1 / var_zero)\n",
    "        complexity = np.sqrt(var_d2 / var_d1) / mobility\n",
    "        return mobility, complexity\n",
    "\n",
    "    def load_preprocessing_artifacts(self, scaler_path, label_encoder_path):\n",
    "        \"\"\"Load preprocessing artifacts with dimension validation\"\"\"\n",
    "        try:\n",
    "            self.scaler = joblib.load(scaler_path)\n",
    "            print(f\"✓ Scaler loaded successfully (expecting {self.scaler.n_features_in_} features)\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"✗ Failed to load scaler: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            self.scaler = None\n",
    "        try:\n",
    "            self.label_encoder = joblib.load(label_encoder_path)\n",
    "            print(\"✓ Label encoder loaded successfully\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"✗ Failed to load label encoder: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            self.label_encoder = None\n",
    "\n",
    "    def tune_models(self, X_train, y_train):\n",
    "        \"\"\"Hyperparameter tuning using grid search\"\"\"\n",
    "        rf_param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2]\n",
    "        }\n",
    "        rf = RandomForestClassifier(random_state=42)\n",
    "        rf_grid = GridSearchCV(rf, rf_param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "        rf_grid.fit(X_train, y_train)\n",
    "        best_rf = rf_grid.best_estimator_\n",
    "\n",
    "        xgb_param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1]\n",
    "        }\n",
    "        xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "        xgb_grid = GridSearchCV(xgb, xgb_param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "        xgb_grid.fit(X_train, y_train)\n",
    "        best_xgb = xgb_grid.best_estimator_\n",
    "\n",
    "        self.models = {\n",
    "            \"RF\": best_rf,\n",
    "            \"XGB\": best_xgb\n",
    "        }\n",
    "\n",
    "    def run_pipeline(self, config):\n",
    "        results = None\n",
    "        try:\n",
    "            print(\"\\n=== EEG Analysis Pipeline ===\\n\")\n",
    "            # 1. Load data\n",
    "            healthy_data, healthy_labels, h_sfreq = self.load_dataset(\n",
    "                config['healthy_path'],\n",
    "                max_files=config.get('max_files'),\n",
    "                max_duration=config.get('max_duration', None))\n",
    "            patient_data, patient_labels, p_sfreq = self.load_dataset(\n",
    "                config['patient_path'],\n",
    "                is_patient=True,\n",
    "                max_files=config.get('max_files'),\n",
    "                max_duration=config.get('max_duration', None))\n",
    "\n",
    "            # 2. Channel matching\n",
    "            if not config.get('skip_channel_matching', False):\n",
    "                healthy_channels = self.collect_channel_names(config['healthy_path'])\n",
    "                patient_channels = self.collect_channel_names(config['patient_path'], is_patient=True)\n",
    "                self.common_channels = self.find_common_channels(healthy_channels, patient_channels)\n",
    "\n",
    "            # 3. Preprocessing\n",
    "            X_healthy, y_healthy = self.preprocess_data(\n",
    "                healthy_data, healthy_labels,\n",
    "                sfreq=h_sfreq,\n",
    "                common_channels=self.common_channels)\n",
    "            X_patients, y_patients = self.preprocess_data(\n",
    "                patient_data, patient_labels,\n",
    "                sfreq=p_sfreq,\n",
    "                common_channels=self.common_channels)\n",
    "\n",
    "            # 4. Apply transformations\n",
    "            self.scaler = StandardScaler()\n",
    "            X = np.vstack([X_healthy, X_patients])\n",
    "            y = np.hstack([y_healthy, y_patients])\n",
    "            X = self.scaler.fit_transform(X)\n",
    "\n",
    "            # 5. Handle class imbalance\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "            # 6. Dimensionality reduction\n",
    "            pca = PCA(n_components=0.95, random_state=42)\n",
    "            X_reduced = pca.fit_transform(X_resampled)\n",
    "\n",
    "            # 7. Train-test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_reduced, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "            # 8. Model tuning\n",
    "            self.tune_models(X_train, y_train)\n",
    "\n",
    "            # 9. Evaluate models\n",
    "            self.evaluate_models(X_test, y_test, data_type='Patient')\n",
    "\n",
    "            # 10. Generate visualizations\n",
    "            self.plot_model_performance_comparison()\n",
    "\n",
    "            print(\"\\n=== Pipeline Completed ===\\n\")\n",
    "            if self._error_messages:\n",
    "                print(\"Completed with warnings/errors (see above messages)\")\n",
    "            else:\n",
    "                print(\"Completed successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n!!! Pipeline Failed !!!\\nError: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            gc.collect()\n",
    "        return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    channel_mapping = {\n",
    "        'FP1': 'Fp1',\n",
    "        'FP2': 'Fp2',\n",
    "        'F3': 'F3',\n",
    "        'F4': 'F4',\n",
    "    }\n",
    "    config = {\n",
    "        'healthy_path': \"F:/shivani/VSCode/ml/worked on dataset/3(final)/Healthy\",\n",
    "        'patient_path': \"F:/shivani/VSCode/ml/worked on dataset/4/dataset/Patients\",\n",
    "        'model_paths': {},\n",
    "        'scaler_path': \"F:/shivani/VSCode/ml/worked on dataset/3(final)/preprocessing_artifacts/eeg_scaler.joblib\",\n",
    "        'label_encoder_path': \"F:/shivani/VSCode/ml/worked on dataset/3(final)/preprocessing_artifacts/label_encoder.joblib\",\n",
    "        'max_files': 5,\n",
    "    }\n",
    "    pipeline = EEGPipeline()\n",
    "    pipeline.set_channel_mapping(channel_mapping)\n",
    "    results = pipeline.run_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 287 (2664835516.py, line 288)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 288\u001b[1;36m\u001b[0m\n\u001b[1;33m    try:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 287\n"
     ]
    }
   ],
   "source": [
    "import os   #alallalaallalalalala\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import mne\n",
    "import gc\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from scipy.signal import welch\n",
    "import pywt\n",
    "import pandas as pd\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# Configure plotting\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "class EEGPipeline:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.scaler = None\n",
    "        self.label_encoder = None\n",
    "        self.common_channels = None\n",
    "        self.feature_names = [\n",
    "            'Delta Power', 'Theta Power', 'Alpha Power', 'Beta Power', 'Gamma Power',\n",
    "            'Wavelet Mean 1', 'Wavelet Mean 2', 'Wavelet Mean 3', 'Wavelet Mean 4', 'Wavelet Mean 5',\n",
    "            'Mean', 'Std Dev', 'Median'\n",
    "        ]\n",
    "        self.band_names = ['Delta', 'Theta', 'Alpha', 'Beta', 'Gamma']\n",
    "        self.channel_mapping = {}\n",
    "        self.expected_features_per_channel = 13\n",
    "        self._error_messages = []  # Track errors during pipeline execution\n",
    "\n",
    "    def set_channel_mapping(self, mapping_dict):\n",
    "        \"\"\"Set manual channel name mapping between different naming conventions\"\"\"\n",
    "        self.channel_mapping = mapping_dict\n",
    "\n",
    "    def normalize_channel_name(self, channel_name):\n",
    "        \"\"\"Advanced channel name normalization with manual mapping support\"\"\"\n",
    "        if isinstance(channel_name, (list, np.ndarray)):\n",
    "            channel_name = channel_name[0]\n",
    "        channel_name = str(channel_name).strip().upper()\n",
    "        \n",
    "        if channel_name in self.channel_mapping:\n",
    "            return self.channel_mapping[channel_name]\n",
    "        \n",
    "        channel_name = re.sub(r'[^A-Z0-9]', '', channel_name)\n",
    "        channel_name = re.sub(r'^CH', '', channel_name)\n",
    "        channel_name = re.sub(r'^EEG', '', channel_name)\n",
    "        channel_name = channel_name.lstrip('0')\n",
    "        \n",
    "        # Handle common variations\n",
    "        variations = {\n",
    "            'FP1': 'Fp1', 'FP2': 'Fp2',\n",
    "            'T3': 'T7', 'T4': 'T8',\n",
    "            'T5': 'P7', 'T6': 'P8'\n",
    "        }\n",
    "        return variations.get(channel_name, channel_name)\n",
    "\n",
    "    def get_channel_names_from_mat(self, mat_path):\n",
    "        \"\"\"Robust MAT file channel extraction supporting multiple formats\"\"\"\n",
    "        try:\n",
    "            mat_data = loadmat(mat_path)\n",
    "            channels = []\n",
    "            \n",
    "            # Structure 1: Nested 'data' structure\n",
    "            if 'data' in mat_data:\n",
    "                data_struct = mat_data['data'][0][0]\n",
    "                if 'channels' in data_struct.dtype.names:\n",
    "                    channels = [str(ch[0]) for ch in data_struct['channels'][0]]\n",
    "                elif 'chanlocs' in data_struct.dtype.names:\n",
    "                    chanlocs = data_struct['chanlocs'][0]\n",
    "                    channels = [str(chan['labels'][0]) for chan in chanlocs]\n",
    "            \n",
    "            # Structure 2: EEGLAB structure\n",
    "            elif 'EEG' in mat_data:\n",
    "                eeg_struct = mat_data['EEG'][0][0]\n",
    "                if 'chanlocs' in eeg_struct.dtype.names:\n",
    "                    chanlocs = eeg_struct['chanlocs'][0]\n",
    "                    channels = [str(chan['labels'][0]) for chan in chanlocs]\n",
    "                elif 'chaninfo' in eeg_struct.dtype.names:\n",
    "                    chaninfo = eeg_struct['chaninfo'][0][0]\n",
    "                    if 'labels' in chaninfo.dtype.names:\n",
    "                        channels = [str(ch[0]) for ch in chaninfo['labels'][0]]\n",
    "            \n",
    "            # Structure 3: Simple X,y structure\n",
    "            elif 'X' in mat_data and 'ch_names' in mat_data:\n",
    "                channels = [str(ch[0]) for ch in mat_data['ch_names'][0]]\n",
    "                \n",
    "            return [self.normalize_channel_name(ch) for ch in channels if ch and str(ch).strip()]\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading {mat_path}: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return []\n",
    "\n",
    "    def get_channel_names_from_vhdr(self, vhdr_path):\n",
    "        \"\"\"Extract channel names from BrainVision files with validation\"\"\"\n",
    "        try:\n",
    "            raw = mne.io.read_raw_brainvision(vhdr_path, preload=False, verbose=False)\n",
    "            return [self.normalize_channel_name(ch) for ch in raw.ch_names]\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading {vhdr_path}: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return []\n",
    "\n",
    "    def find_common_channels(self, healthy_channels, patient_channels):\n",
    "        \"\"\"Flexible channel matching with multiple strategies\"\"\"\n",
    "        # First try exact matching\n",
    "        common = set(healthy_channels).intersection(patient_channels)\n",
    "        \n",
    "        if not common:\n",
    "            healthy_set = set(healthy_channels)\n",
    "            patient_set = set(patient_channels)\n",
    "            common = healthy_set.intersection(patient_set)\n",
    "            \n",
    "        if not common:\n",
    "            common_partial = set()\n",
    "            for h_ch in healthy_set:\n",
    "                for p_ch in patient_set:\n",
    "                    if h_ch in p_ch or p_ch in h_ch:\n",
    "                        common_partial.add(h_ch)\n",
    "            if common_partial:\n",
    "                print(f\"Using partial channel matches: {common_partial}\")\n",
    "                return sorted(common_partial)\n",
    "            \n",
    "        print(f\"Found {len(common)} common channels\")\n",
    "        return sorted(common)\n",
    "\n",
    "    def collect_channel_names(self, folder_path, is_patient=False):\n",
    "        \"\"\"Collect channel names with extensive validation\"\"\"\n",
    "        all_channels = set()\n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith('.vhdr' if not is_patient else '.mat')]\n",
    "        \n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No valid files found in {folder_path}\")\n",
    "            \n",
    "        for file in tqdm(files, desc=f\"Collecting {'patient' if is_patient else 'healthy'} channels\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            channels = self.get_channel_names_from_mat(file_path) if is_patient else self.get_channel_names_from_vhdr(file_path)\n",
    "            \n",
    "            if not channels:\n",
    "                print(f\"Warning: No channels found in {file}\")\n",
    "                continue\n",
    "                \n",
    "            all_channels.update(channels)\n",
    "            \n",
    "        if not all_channels:\n",
    "            raise ValueError(f\"No channels collected from {folder_path}\")\n",
    "            \n",
    "        return sorted(all_channels)\n",
    "\n",
    "    def load_dataset(self, folder_path, is_patient=False, max_files=None, max_duration=None):\n",
    "        \"\"\"Robust dataset loading with comprehensive validation\"\"\"\n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith('.mat' if is_patient else '.vhdr')]\n",
    "        loader_func = self.load_mat_data if is_patient else self.load_brainvision_data\n",
    "            \n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No valid files found in {folder_path}\")\n",
    "            \n",
    "        if max_files:\n",
    "            files = files[:max_files]\n",
    "            \n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        sfreqs = []\n",
    "        loaded_files = 0\n",
    "        \n",
    "        for file in tqdm(files, desc=f\"Loading {'patient' if is_patient else 'healthy'} files\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            try:\n",
    "                data, labels, sfreq = loader_func(file_path, max_duration)\n",
    "                \n",
    "                if len(data) > 0 and len(labels) > 0:\n",
    "                    all_data.append(data)\n",
    "                    all_labels.append(labels)\n",
    "                    sfreqs.append(sfreq)\n",
    "                    loaded_files += 1\n",
    "                else:\n",
    "                    print(f\"Skipping {file} - no valid data\")\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error loading {file}: {str(e)}\"\n",
    "                self._error_messages.append(error_msg)\n",
    "                print(error_msg)\n",
    "                \n",
    "            gc.collect()\n",
    "            \n",
    "        print(f\"\\nSuccessfully loaded {loaded_files}/{len(files)} files\")\n",
    "        \n",
    "        if not all_data:\n",
    "            raise ValueError(\"No valid data loaded - check file formats\")\n",
    "            \n",
    "        avg_sfreq = np.mean(sfreqs) if sfreqs else (100 if is_patient else 250)\n",
    "        return np.concatenate(all_data), np.concatenate(all_labels), avg_sfreq\n",
    "\n",
    "    def load_brainvision_data(self, vhdr_path, max_duration=None):\n",
    "        \"\"\"Load BrainVision data with enhanced validation\"\"\"\n",
    "        try:\n",
    "            raw = mne.io.read_raw_brainvision(vhdr_path, preload=True, verbose=False)\n",
    "            \n",
    "            if max_duration:\n",
    "                crop_end = min(max_duration, raw.times[-1])\n",
    "                raw.crop(tmax=crop_end)\n",
    "                \n",
    "            data = raw.get_data().T.astype(np.float32)\n",
    "            events, _ = mne.events_from_annotations(raw)\n",
    "            labels = events[:, 2] if len(events) > 0 else np.zeros(len(data))\n",
    "            \n",
    "            return data, labels, raw.info['sfreq']\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading {vhdr_path}: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return np.array([]), np.array([]), None\n",
    "\n",
    "    def load_mat_data(self, mat_path, max_duration=None, default_sfreq=100):\n",
    "        \"\"\"Flexible MAT file loader supporting multiple structures\"\"\"\n",
    "        try:\n",
    "            mat_data = loadmat(mat_path)\n",
    "            eeg_data = None\n",
    "            labels = None\n",
    "            sfreq = default_sfreq\n",
    "            \n",
    "            if 'data' in mat_data:\n",
    "                data_struct = mat_data['data'][0][0]\n",
    "                if 'X' in data_struct.dtype.names:\n",
    "                    eeg_data = data_struct['X']\n",
    "                    if eeg_data.ndim > 2:\n",
    "                        eeg_data = eeg_data.reshape(eeg_data.shape[0], -1)\n",
    "                    if 'y' in data_struct.dtype.names:\n",
    "                        labels = data_struct['y'].flatten()\n",
    "                    if 'sfreq' in data_struct.dtype.names:\n",
    "                        sfreq = float(data_struct['sfreq'][0][0])\n",
    "                    elif 'Fs' in data_struct.dtype.names:\n",
    "                        sfreq = float(data_struct['Fs'][0][0])\n",
    "            \n",
    "            elif 'EEG' in mat_data:\n",
    "                eeg_struct = mat_data['EEG'][0][0]\n",
    "                if 'data' in eeg_struct.dtype.names:\n",
    "                    eeg_data = eeg_struct['data'].T\n",
    "                if 'event' in eeg_struct.dtype.names:\n",
    "                    events = eeg_struct['event'][0]\n",
    "                    labels = np.array([ev[0]['type'][0] for ev in events])\n",
    "                if 'srate' in eeg_struct.dtype.names:\n",
    "                    sfreq = float(eeg_struct['srate'][0][0])\n",
    "            \n",
    "            elif 'X' in mat_data:\n",
    "                eeg_data = mat_data['X']\n",
    "                if 'y' in mat_data:\n",
    "                    labels = mat_data['y'].flatten()\n",
    "            \n",
    "            if labels is None or len(np.unique(labels)) <= 1:\n",
    "                labels = np.zeros(len(eeg_data)) if eeg_data is not None else np.array([])\n",
    "            \n",
    "            if eeg_data is None:\n",
    "                raise ValueError(\"No EEG data found in MAT file\")\n",
    "            \n",
    "            min_len = min(len(eeg_data), len(labels))\n",
    "            eeg_data = eeg_data[:min_len]\n",
    "            labels = labels[:min_len]\n",
    "            \n",
    "            if max_duration:\n",
    "                max_samples = int(max_duration * sfreq)\n",
    "                eeg_data = eeg_data[:max_samples]\n",
    "                labels = labels[:max_samples]\n",
    "                \n",
    "            return eeg_data.astype(np.float32), labels, sfreq\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading {mat_path}: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return np.array([]), np.array([]), None\n",
    "\n",
    "    def preprocess_data(self, data, labels, sfreq=250, common_channels=None):\n",
    "    try:\n",
    "        # Validate input\n",
    "        if data is None or len(data) == 0:\n",
    "            raise ValueError(\"Empty data array\")\n",
    "        \n",
    "        n_channels = data.shape[1]\n",
    "        ch_names = common_channels[:n_channels] if common_channels and len(common_channels) >= n_channels else [f'ch{i}' for i in range(n_channels)]\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')\n",
    "        raw = mne.io.RawArray(data.T, info)\n",
    "        \n",
    "        # Apply filters with safety checks\n",
    "        nyquist = sfreq / 2\n",
    "        raw.filter(0.5, min(40, nyquist - 1), fir_design='firwin', phase='zero-double')\n",
    "        notch_freqs = [50, 60]  # Default frequencies\n",
    "        notch_freqs = [f for f in notch_freqs if f < nyquist]  # Filter out invalid freqs\n",
    "        if notch_freqs:\n",
    "            raw.notch_filter(notch_freqs)\n",
    "        \n",
    "        # Create epochs\n",
    "        events = mne.make_fixed_length_events(raw, duration=1.0)\n",
    "        epochs = mne.Epochs(raw, events, tmin=0, tmax=1.0, baseline=None, preload=True)\n",
    "        epochs_data = epochs.get_data()\n",
    "\n",
    "        def extract_features(epoch_data):\n",
    "            features = []\n",
    "            for epoch in epoch_data:\n",
    "                epoch_features = []\n",
    "                for channel in epoch:\n",
    "                    # Frequency features\n",
    "                    freqs, psd = welch(channel, fs=sfreq, nperseg=min(256, len(channel)))\n",
    "                    bands = {'delta': (0.5, 4), 'theta': (4, 8), 'alpha': (8, 12),\n",
    "                             'beta': (12, 30), 'gamma': (30, min(50, nyquist - 1))}\n",
    "                    band_powers = [np.sum(psd[(freqs >= low) & (freqs <= high)]) \n",
    "                                   for low, high in bands.values()]\n",
    "                    \n",
    "                    # Wavelet features\n",
    "                    coeffs = pywt.wavedec(channel, 'db4', level=4)\n",
    "                    wavelet_features = [np.mean(c) for c in coeffs[:5]]\n",
    "                    if len(wavelet_features) < 5:\n",
    "                        wavelet_features += [0.0] * (5 - len(wavelet_features))\n",
    "                    \n",
    "                    # Statistical features\n",
    "                    stats = [\n",
    "                        np.mean(channel), \n",
    "                        np.std(channel), \n",
    "                        np.median(channel),\n",
    "                        pd.Series(channel).skew(),  # Skewness\n",
    "                        pd.Series(channel).kurtosis()  # Kurtosis\n",
    "                    ]\n",
    "                    \n",
    "                    # Hjorth parameters\n",
    "                    mobility, complexity = self.hjorth_parameters(channel)\n",
    "                    \n",
    "                    # Spectral entropy\n",
    "                    spectral_entropy = -np.sum(psd * np.log(psd + 1e-10))\n",
    "                    \n",
    "                    # Zero-crossing rate\n",
    "                    zero_crossings = np.where(np.diff(np.sign(channel)))[0].size\n",
    "                    \n",
    "                    # Peak-to-peak amplitude\n",
    "                    peak_to_peak = np.max(channel) - np.min(channel)\n",
    "                    \n",
    "                    epoch_features.extend(\n",
    "                        band_powers + wavelet_features + stats + \n",
    "                        [mobility, complexity, spectral_entropy, zero_crossings, peak_to_peak]\n",
    "                    )\n",
    "                features.append(epoch_features)\n",
    "            return np.array(features)\n",
    "        \n",
    "        X = extract_features(epochs_data)\n",
    "        y = labels[:len(X)]\n",
    "        \n",
    "        expected_features = n_channels * self.expected_features_per_channel\n",
    "        if X.shape[1] != expected_features:\n",
    "            if X.shape[1] < expected_features:\n",
    "                pad_width = ((0, 0), (0, expected_features - X.shape[1]))\n",
    "                X = np.pad(X, pad_width, mode='constant')\n",
    "            else:\n",
    "                X = X[:, :expected_features]\n",
    "        return X, y\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error during preprocessing (sfreq={sfreq}): {str(e)}\"\n",
    "        self._error_messages.append(error_msg)\n",
    "        print(error_msg)\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    def load_preprocessing_artifacts(self, scaler_path, label_encoder_path):\n",
    "        \"\"\"Load preprocessing artifacts with dimension validation\"\"\"\n",
    "        try:\n",
    "            self.scaler = joblib.load(scaler_path)\n",
    "            print(f\"✓ Scaler loaded successfully (expecting {self.scaler.n_features_in_} features)\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"✗ Failed to load scaler: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            self.scaler = None\n",
    "            \n",
    "        try:\n",
    "            self.label_encoder = joblib.load(label_encoder_path)\n",
    "            print(\"✓ Label encoder loaded successfully\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"✗ Failed to load label encoder: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            self.label_encoder = None\n",
    "\n",
    "    def match_features(self, X, expected_features):\n",
    "        \"\"\"Ensure feature matrix matches expected dimensions\"\"\"\n",
    "        if X.shape[1] == expected_features:\n",
    "            return X\n",
    "        elif X.shape[1] < expected_features:\n",
    "            pad_width = ((0, 0), (0, expected_features - X.shape[1]))\n",
    "            return np.pad(X, pad_width, mode='constant')\n",
    "        else:\n",
    "            return X[:, :expected_features]\n",
    "\n",
    "    def load_models(self, model_paths):\n",
    "        \"\"\"Load models with comprehensive validation\"\"\"\n",
    "        self.models = {}\n",
    "        for name, path in model_paths.items():\n",
    "            try:\n",
    "                self.models[name] = joblib.load(path)\n",
    "                print(f\"✓ {name} model loaded successfully\")\n",
    "            except Exception as e:\n",
    "                error_msg = f\"✗ Failed to load {name} model: {str(e)}\"\n",
    "                self._error_messages.append(error_msg)\n",
    "                print(error_msg)\n",
    "                \n",
    "        if not self.models:\n",
    "            raise ValueError(\"No models were loaded successfully\")\n",
    "        return self.models\n",
    "\n",
    "    def evaluate_models(self, X_data, y_data, data_type='Patient'):\n",
    "        \"\"\"Updated evaluation method with multiclass support and custom accuracy threshold\"\"\"\n",
    "        self.results[data_type] = {}\n",
    "        for name, model in self.models.items():\n",
    "            try:\n",
    "                y_pred = model.predict(X_data)\n",
    "                \n",
    "                if len(np.unique(y_data)) > 2:\n",
    "                    acc = accuracy_score(y_data, y_pred)\n",
    "                    report = classification_report(y_data, y_pred, output_dict=True, zero_division=0)\n",
    "                    cm = confusion_matrix(y_data, y_pred)\n",
    "                    roc_auc = None\n",
    "                else:\n",
    "                    y_proba = model.predict_proba(X_data)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "                    acc = accuracy_score(y_data, y_pred)\n",
    "                    report = classification_report(y_data, y_pred, output_dict=True, zero_division=0)\n",
    "                    cm = confusion_matrix(y_data, y_pred)\n",
    "                    if y_proba is not None:\n",
    "                        fpr, tpr, _ = roc_curve(y_data, y_proba)\n",
    "                        roc_auc = auc(fpr, tpr)\n",
    "                    else:\n",
    "                        roc_auc = None\n",
    "                \n",
    "                # Check if accuracy meets the custom threshold of 65%\n",
    "                meets_custom_threshold = acc >= 0.65\n",
    "                \n",
    "                self.results[data_type][name] = {\n",
    "                    'accuracy': acc,\n",
    "                    'meets_custom_threshold': meets_custom_threshold,  # New field for custom threshold\n",
    "                    'report': report,\n",
    "                    'confusion_matrix': cm,\n",
    "                    'roc_auc': roc_auc,\n",
    "                    'fpr': fpr if 'fpr' in locals() else None,\n",
    "                    'tpr': tpr if 'tpr' in locals() else None,\n",
    "                    'y_true': y_data,\n",
    "                    'y_pred': y_pred\n",
    "                }\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error evaluating {name} on {data_type} data: {str(e)}\"\n",
    "                self._error_messages.append(error_msg)\n",
    "                print(error_msg)\n",
    "    def plot_eeg_comparison(self, healthy_data, patient_data, healthy_sfreq=250, patient_sfreq=100, samples=500, channels=3):\n",
    "        \"\"\"Enhanced horizontal EEG signal comparison plot\"\"\"\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), sharey=True)\n",
    "        \n",
    "        time_h = np.arange(min(samples, len(healthy_data))) / healthy_sfreq\n",
    "        time_p = np.arange(min(samples, len(patient_data))) / patient_sfreq\n",
    "        \n",
    "        # Plot healthy EEG\n",
    "        for ch in range(min(channels, healthy_data.shape[1])):\n",
    "            ax1.plot(time_h, healthy_data[:len(time_h), ch] * 1e6,\n",
    "                    linewidth=2, alpha=0.8, label=f'Channel {ch+1}')\n",
    "        \n",
    "        ax1.set_title('Healthy EEG Signals', fontsize=16, pad=20)\n",
    "        ax1.set_xlabel('Time (seconds)', fontsize=14)\n",
    "        ax1.set_ylabel('Amplitude (μV)', fontsize=14)\n",
    "        ax1.legend(loc='upper right', fontsize=12)\n",
    "        ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax1.set_ylim(-100, 100)\n",
    "        ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "        \n",
    "        # Plot patient EEG\n",
    "        for ch in range(min(channels, patient_data.shape[1])):\n",
    "            ax2.plot(time_p, patient_data[:len(time_p), ch] * 1e6,\n",
    "                    linewidth=2, alpha=0.8, label=f'Channel {ch+1}')\n",
    "        \n",
    "        ax2.set_title('Patient EEG Signals', fontsize=16, pad=20)\n",
    "        ax2.set_xlabel('Time (seconds)', fontsize=14)\n",
    "        ax2.legend(loc='upper right', fontsize=12)\n",
    "        ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax2.set_ylim(-100, 100)\n",
    "        ax2.tick_params(axis='both', which='major', labelsize=12)\n",
    "        \n",
    "        plt.suptitle('EEG Signal Comparison (First 500 Samples)', fontsize=18, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_patient_response_categories(self):\n",
    "        \"\"\"Enhanced patient response categorization visualization with robust error handling\"\"\"\n",
    "        if not self.results or 'Patient' not in self.results:\n",
    "            print(\"No patient results available for visualization\")\n",
    "            return\n",
    "    \n",
    "        try:\n",
    "            # Use the first available model's results\n",
    "            model_name = next(iter(self.results['Patient']))\n",
    "            results = self.results['Patient'][model_name]\n",
    "            \n",
    "            if 'y_true' not in results or 'y_pred' not in results:\n",
    "                print(\"Missing required data in results\")\n",
    "                return\n",
    "                \n",
    "            y_true = results['y_true']\n",
    "            y_pred = results['y_pred']\n",
    "            \n",
    "            # Calculate accuracy for each sample\n",
    "            if len(np.unique(y_true)) == 2:\n",
    "                # Binary classification\n",
    "                accuracies = (y_pred == y_true).astype(float)\n",
    "            else:\n",
    "                # Multiclass classification\n",
    "                accuracies = np.array([1.0 if pred == true else 0.0 \n",
    "                                     for pred, true in zip(y_pred, y_true)])\n",
    "            \n",
    "            # Categorize patients with safe division\n",
    "            categories = []\n",
    "            for acc in accuracies:\n",
    "                if acc >= 0.7:\n",
    "                    categories.append(\"Good Response\")\n",
    "                elif 0.4 <= acc < 0.7:\n",
    "                    categories.append(\"Medium Response\")\n",
    "                else:\n",
    "                    categories.append(\"Poor Response\")\n",
    "            \n",
    "            category_counts = pd.Series(categories).value_counts()\n",
    "            \n",
    "            # Create figure with proper layout\n",
    "            fig = plt.figure(figsize=(18, 8))\n",
    "            gs = GridSpec(1, 2, width_ratios=[1, 1.5])\n",
    "            \n",
    "            # Subplot 1: Pie chart with safe explode parameter\n",
    "            ax1 = fig.add_subplot(gs[0])\n",
    "            colors = ['#4CAF50', '#FFC107', '#F44336']\n",
    "            \n",
    "            # Ensure explode matches the number of categories\n",
    "            explode = (0.05, 0.05, 0.05)[:len(category_counts)]\n",
    "            \n",
    "            # Handle case where we might have fewer than 3 categories\n",
    "            if len(category_counts) < 3:\n",
    "                colors = colors[:len(category_counts)]\n",
    "                explode = explode[:len(category_counts)]\n",
    "            \n",
    "            wedges, texts, autotexts = ax1.pie(\n",
    "                category_counts, \n",
    "                labels=category_counts.index, \n",
    "                autopct=lambda p: f'{p:.1f}%' if p > 0 else '',\n",
    "                startangle=90, \n",
    "                colors=colors,\n",
    "                explode=explode,\n",
    "                textprops={'fontsize': 12}\n",
    "            )\n",
    "            \n",
    "            for autotext in autotexts:\n",
    "                autotext.set_color('white')\n",
    "                autotext.set_fontweight('bold')\n",
    "            \n",
    "            ax1.set_title('Patient Response Distribution', fontsize=16, pad=20)\n",
    "            \n",
    "            # Subplot 2: Bar plot\n",
    "            ax2 = fig.add_subplot(gs[1])\n",
    "            barplot = sns.barplot(\n",
    "                x=category_counts.index, \n",
    "                y=category_counts.values, \n",
    "                ax=ax2,\n",
    "                palette=colors,\n",
    "                saturation=0.8\n",
    "            )\n",
    "            \n",
    "            ax2.set_title('Patient Response Categories', fontsize=16, pad=20)\n",
    "            ax2.set_xlabel('Response Category', fontsize=14)\n",
    "            ax2.set_ylabel('Number of Patients', fontsize=14)\n",
    "            \n",
    "            # Add count annotations\n",
    "            for p in barplot.patches:\n",
    "                height = p.get_height()\n",
    "                if not np.isnan(height) and height > 0:\n",
    "                    barplot.annotate(\n",
    "                        f'{int(height)}',\n",
    "                        (p.get_x() + p.get_width() / 2., height),\n",
    "                        ha='center', va='center',\n",
    "                        xytext=(0, 10),\n",
    "                        textcoords='offset points',\n",
    "                        fontsize=12,\n",
    "                        fontweight='bold'\n",
    "                    )\n",
    "            \n",
    "            # Add overall accuracy if available\n",
    "            if 'accuracy' in results:\n",
    "                overall_acc = results['accuracy']\n",
    "                fig.text(\n",
    "                    0.5, -0.05,\n",
    "                    f'Model: {model_name} | Overall Accuracy: {overall_acc:.1%}',\n",
    "                    ha='center', va='center', fontsize=14\n",
    "                )\n",
    "            \n",
    "            plt.suptitle('Patient Response Categorization', fontsize=18, y=1.05)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating patient response visualization: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def plot_confusion_matrices(self, data_type='Patient'):\n",
    "        \"\"\"Enhanced confusion matrix visualization with better formatting\"\"\"\n",
    "        if not self.results or data_type not in self.results:\n",
    "            print(f\"No results available for {data_type} data\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            models = list(self.results[data_type].keys())\n",
    "            num_models = len(models)\n",
    "            \n",
    "            if num_models == 0:\n",
    "                print(\"No models available for visualization\")\n",
    "                return\n",
    "                \n",
    "            # Create figure with appropriate size\n",
    "            fig, axes = plt.subplots(1, num_models, figsize=(6*num_models, 5))\n",
    "            if num_models == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for i, model_name in enumerate(models):\n",
    "                model_results = self.results[data_type][model_name]\n",
    "                \n",
    "                if 'confusion_matrix' not in model_results:\n",
    "                    print(f\"No confusion matrix for {model_name}\")\n",
    "                    continue\n",
    "                    \n",
    "                cm = model_results['confusion_matrix']\n",
    "                \n",
    "                # Get class names\n",
    "                if self.label_encoder:\n",
    "                    classes = self.label_encoder.classes_\n",
    "                else:\n",
    "                    # Handle binary and multiclass cases\n",
    "                    n_classes = cm.shape[0]\n",
    "                    classes = [f'Class {i}' for i in range(n_classes)]\n",
    "                    if n_classes == 2:\n",
    "                        classes = ['Negative', 'Positive']\n",
    "                \n",
    "                # Normalize the confusion matrix\n",
    "                cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                \n",
    "                # Plot with annotations\n",
    "                sns.heatmap(\n",
    "                    cm_normalized, \n",
    "                    annot=True, \n",
    "                    fmt='.2f',\n",
    "                    cmap='Blues',\n",
    "                    xticklabels=classes,\n",
    "                    yticklabels=classes,\n",
    "                    ax=axes[i],\n",
    "                    cbar=False,\n",
    "                    annot_kws={'fontsize': 10},\n",
    "                    vmin=0, vmax=1\n",
    "                )\n",
    "                \n",
    "                axes[i].set_title(f'{model_name}\\nConfusion Matrix', fontsize=14)\n",
    "                axes[i].set_xlabel('Predicted Label', fontsize=12)\n",
    "                axes[i].set_ylabel('True Label', fontsize=12)\n",
    "            \n",
    "            plt.suptitle(f'Model Performance on {data_type} Data', fontsize=16, y=1.05)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating confusion matrices: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    def plot_roc_curves_comparison(self):\n",
    "        \"\"\"Plot ROC curves for all models for comparison\"\"\"\n",
    "        if not self.results or 'Patient' not in self.results:\n",
    "            print(\"No patient results available\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for model_name, results in self.results['Patient'].items():\n",
    "            if results.get('roc_auc') is not None:\n",
    "                plt.plot(results['fpr'], results['tpr'],\n",
    "                        label=f'{model_name} (AUC = {results[\"roc_auc\"]:.2f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic Comparison')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_model_performance_comparison(self):\n",
    "        \"\"\"Enhanced model performance comparison visualization\"\"\"\n",
    "        if not self.results or 'Patient' not in self.results:\n",
    "            raise ValueError(\"Patient results not available\")\n",
    "            \n",
    "        models = list(self.results['Patient'].keys())\n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1-score']\n",
    "        \n",
    "        metrics_data = []\n",
    "        for model in models:\n",
    "            report = self.results['Patient'][model]['report']\n",
    "            if isinstance(report, dict) and 'accuracy' in report:\n",
    "                if 'macro avg' in report:\n",
    "                    metrics_data.append({\n",
    "                        'Model': model,\n",
    "                        'Accuracy': report['accuracy'],\n",
    "                        'Precision': report['macro avg']['precision'],\n",
    "                        'Recall': report['macro avg']['recall'],\n",
    "                        'F1-Score': report['macro avg']['f1-score']\n",
    "                    })\n",
    "                elif len(report.keys()) > 3:\n",
    "                    metrics_data.append({\n",
    "                        'Model': model,\n",
    "                        'Accuracy': report['accuracy'],\n",
    "                        'Precision': report['1']['precision'],\n",
    "                        'Recall': report['1']['recall'],\n",
    "                        'F1-Score': report['1']['f1-score']\n",
    "                    })\n",
    "        \n",
    "        if not metrics_data:\n",
    "            raise ValueError(\"No valid metric data found\")\n",
    "            \n",
    "        df = pd.DataFrame(metrics_data)\n",
    "        df_melted = df.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        barplot = sns.barplot(\n",
    "            x='Model', \n",
    "            y='Score', \n",
    "            hue='Metric', \n",
    "            data=df_melted,\n",
    "            palette='viridis',\n",
    "            alpha=0.8\n",
    "        )\n",
    "        \n",
    "        plt.title('Model Performance Metrics Comparison', fontsize=16, pad=20)\n",
    "        plt.xlabel('Model', fontsize=14)\n",
    "        plt.ylabel('Score', fontsize=14)\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        for p in barplot.patches:\n",
    "            barplot.annotate(\n",
    "                format(p.get_height(), '.2f'),\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center',\n",
    "                xytext=(0, 10),\n",
    "                textcoords='offset points',\n",
    "                fontsize=10\n",
    "            )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def _get_error_messages(self):\n",
    "        \"\"\"Helper to collect error messages\"\"\"\n",
    "        return self._error_messages\n",
    "\n",
    "    def run_pipeline(self, config):\n",
    "        \"\"\"Complete EEG analysis pipeline with robust error handling\"\"\"\n",
    "        results = None\n",
    "        try:\n",
    "            print(\"\\n=== EEG Analysis Pipeline ===\\n\")\n",
    "            \n",
    "            # 1. Load data\n",
    "            print(\"[1/7] Loading data...\")\n",
    "            healthy_data, healthy_labels, h_sfreq = self.load_dataset(\n",
    "                config['healthy_path'],\n",
    "                max_files=config.get('max_files'),\n",
    "                max_duration=config.get('max_duration', None))\n",
    "            patient_data, patient_labels, p_sfreq = self.load_dataset(\n",
    "                config['patient_path'],\n",
    "                is_patient=True,\n",
    "                max_files=config.get('max_files'),\n",
    "                max_duration=config.get('max_duration', None))\n",
    "            \n",
    "            # 2. Channel matching\n",
    "            print(\"[2/7] Finding common channels...\")\n",
    "            if not config.get('skip_channel_matching', False):\n",
    "                healthy_channels = self.collect_channel_names(config['healthy_path'])\n",
    "                patient_channels = self.collect_channel_names(config['patient_path'], is_patient=True)\n",
    "                self.common_channels = self.find_common_channels(healthy_channels, patient_channels)\n",
    "            \n",
    "            # 3. Preprocessing\n",
    "            print(\"[3/7] Preprocessing data...\")\n",
    "            X_healthy, y_healthy = self.preprocess_data(\n",
    "                healthy_data, healthy_labels,\n",
    "                sfreq=h_sfreq,\n",
    "                common_channels=self.common_channels)\n",
    "            X_patients, y_patients = self.preprocess_data(\n",
    "                patient_data, patient_labels,\n",
    "                sfreq=p_sfreq,\n",
    "                common_channels=self.common_channels)\n",
    "            \n",
    "            # 4. Load artifacts\n",
    "            print(\"[4/7] Loading preprocessing artifacts...\")\n",
    "            self.load_preprocessing_artifacts(\n",
    "                config['scaler_path'],\n",
    "                config['label_encoder_path'])\n",
    "            \n",
    "            # 5. Apply transformations\n",
    "            print(\"[5/7] Applying transformations...\")\n",
    "            if self.scaler:\n",
    "                expected_features = self.scaler.n_features_in_\n",
    "                X_healthy = self.match_features(X_healthy, expected_features)\n",
    "                X_patients = self.match_features(X_patients, expected_features)\n",
    "                X_healthy = self.scaler.transform(X_healthy)\n",
    "                X_patients = self.scaler.transform(X_patients)\n",
    "            \n",
    "            if self.label_encoder:\n",
    "                y_patients = self.label_encoder.transform(y_patients)\n",
    "            \n",
    "            # 6. Load models\n",
    "            print(\"[6/7] Loading models...\")\n",
    "            self.load_models(config['model_paths'])\n",
    "            \n",
    "            # 7. Evaluate models (only on patient data)\n",
    "            print(\"[7/7] Evaluating models...\")\n",
    "            self.evaluate_models(X_patients, y_patients, data_type='Patient')\n",
    "            \n",
    "            # Generate visualizations\n",
    "            print(\"\\n=== Generating Visualizations ===\\n\")\n",
    "            try:\n",
    "                plt.style.use('seaborn-v0_8-whitegrid')\n",
    "            except:\n",
    "                plt.style.use('ggplot')\n",
    "            \n",
    "            vis_funcs = [\n",
    "                ('EEG Comparison', self.plot_eeg_comparison, [healthy_data, patient_data, h_sfreq, p_sfreq]),\n",
    "                ('Patient Response', self.plot_patient_response_categories, []),\n",
    "                ('Confusion Matrix', self.plot_confusion_matrices, ['Patient']),\n",
    "                ('Model Performance', self.plot_model_performance_comparison, []),\n",
    "                ('ROC Curve' , self.plot_roc_curves_comparison,[])\n",
    "            ]\n",
    "            \n",
    "            for name, func, args in vis_funcs:\n",
    "                try:\n",
    "                    print(f\"Generating {name} visualization...\")\n",
    "                    func(*args)\n",
    "                    plt.close('all')\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Failed to generate {name}: {str(e)}\"\n",
    "                    self._error_messages.append(error_msg)\n",
    "                    print(error_msg)\n",
    "            \n",
    "            print(\"\\n=== Pipeline Completed ===\\n\")\n",
    "            if self._error_messages:\n",
    "                print(\"Completed with warnings/errors (see above messages)\")\n",
    "            else:\n",
    "                print(\"Completed successfully!\")\n",
    "            \n",
    "            results = {\n",
    "                'healthy_data': (X_healthy, y_healthy),\n",
    "                'patient_data': (X_patients, y_patients),\n",
    "                'results': self.results,\n",
    "                'metadata': {\n",
    "                    'healthy_samples': len(X_healthy),\n",
    "                    'patient_samples': len(X_patients),\n",
    "                    'common_channels': self.common_channels,\n",
    "                    'features_per_channel': self.expected_features_per_channel,\n",
    "                    'errors': self._error_messages\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n!!! Pipeline Failed !!!\\nError: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "        finally:\n",
    "            if 'healthy_data' in locals():\n",
    "                del healthy_data\n",
    "            if 'patient_data' in locals():\n",
    "                del patient_data\n",
    "            gc.collect()\n",
    "            \n",
    "        return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    channel_mapping = {\n",
    "        'FP1': 'Fp1',\n",
    "        'FP2': 'Fp2',\n",
    "        'F3': 'F3',\n",
    "        'F4': 'F4',\n",
    "    }\n",
    "\n",
    "    config = {\n",
    "        'healthy_path': \"F:/shivani/VSCode/ml/worked on dataset/3(final)/Healthy\",\n",
    "        'patient_path': \"F:/shivani/VSCode/ml/worked on dataset/4/dataset/Patients\",\n",
    "        'model_paths': {\n",
    "            \"SVM\": \"F:/shivani/VSCode/ml/worked on dataset/3(final)/svm_model.pkl\",\n",
    "            \"RF\": \"F:/shivani/VSCode/ml/worked on dataset/3(final)/rf_model.pkl\",\n",
    "            \"XGB\": \"F:/shivani/VSCode/ml/worked on dataset/3(final)/xgb_model.pkl\"\n",
    "        },\n",
    "        'scaler_path': \"F:/shivani/VSCode/ml/worked on dataset/3(final)/preprocessing_artifacts/eeg_scaler.joblib\",\n",
    "        'label_encoder_path': \"F:/shivani/VSCode/ml/worked on dataset/3(final)/preprocessing_artifacts/label_encoder.joblib\",\n",
    "        'max_files': 5,\n",
    "    }\n",
    "\n",
    "    pipeline = EEGPipeline()\n",
    "    pipeline.set_channel_mapping(channel_mapping)\n",
    "    results = pipeline.run_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EEG Analysis Pipeline ===\n",
      "\n",
      "[1/7] Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading healthy files:   0%|                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-12 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    5.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    2.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['Comment/no USB Connection to actiCAP', 'New Segment/', 'Stimulus/S  1', 'Stimulus/S200', 'Stimulus/S210']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading healthy files:  20%|████████████▏                                                | 1/5 [00:36<02:24, 36.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-12 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    5.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    2.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['Comment/no USB Connection to actiCAP', 'New Segment/', 'Stimulus/S  1', 'Stimulus/S200', 'Stimulus/S210']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading healthy files:  40%|████████████████████████▍                                    | 2/5 [01:11<01:46, 35.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-12 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    5.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    2.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['Comment/no USB Connection to actiCAP', 'New Segment/', 'Stimulus/S  1', 'Stimulus/S200', 'Stimulus/S210']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading healthy files:  60%|████████████████████████████████████▌                        | 3/5 [01:46<01:11, 35.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-12 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    5.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    2.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['Comment/no USB Connection to actiCAP', 'New Segment/', 'Stimulus/S  1', 'Stimulus/S200', 'Stimulus/S210']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading healthy files:  80%|████████████████████████████████████████████████▊            | 4/5 [02:22<00:35, 35.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-12 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    5.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    2.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['Comment/actiCAP Data On', 'New Segment/', 'Stimulus/S  1', 'Stimulus/S200', 'Stimulus/S210']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading healthy files: 100%|█████████████████████████████████████████████████████████████| 5/5 [02:58<00:00, 35.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded 5/5 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading patient files: 100%|█████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded 5/5 files\n",
      "[2/7] Finding common channels...\n",
      "[3/7] Preprocessing data...\n",
      "\n",
      "=== Starting Preprocessing ===\n",
      "Input data shape: (12704800, 62)\n",
      "Sample rate: 2500.0 Hz\n",
      "Number of channels: 62\n",
      "First 3 channel names: ['ch0', 'ch1', 'ch2']\n",
      "Created info structure\n",
      "Creating RawArray with float64 data, n_channels=62, n_times=12704800\n",
      "    Range : 0 ... 12704799 =      0.000 ...  5081.920 secs\n",
      "Ready.\n",
      "Successfully created RawArray\n",
      "Nyquist frequency: 1250.0 Hz\n",
      "Applying bandpass filter...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-12 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:   29.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying notch filter...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:   19.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating epochs...\n",
      "Created 5081 events\n",
      "Not setting metadata\n",
      "5081 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 5081 events and 2501 original time points ...\n",
      "0 bad epochs dropped\n",
      "Epochs shape: (5081, 62, 2501)\n",
      "Extracting features...\n",
      "Extracted features shape: (5081, 1054)\n",
      "\n",
      "=== Starting Preprocessing ===\n",
      "Input data shape: (1738520, 8)\n",
      "Sample rate: 100.0 Hz\n",
      "Number of channels: 8\n",
      "First 3 channel names: ['ch0', 'ch1', 'ch2']\n",
      "Created info structure\n",
      "Creating RawArray with float64 data, n_channels=8, n_times=1738520\n",
      "    Range : 0 ... 1738519 =      0.000 ... 17385.190 secs\n",
      "Ready.\n",
      "Successfully created RawArray\n",
      "Nyquist frequency: 50.0 Hz\n",
      "Applying bandpass filter...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 5.00 Hz (-12 dB cutoff frequency: 47.50 Hz)\n",
      "- Filter length: 661 samples (6.610 s)\n",
      "\n",
      "Applying notch filter...\n",
      "Creating epochs...\n",
      "Created 17385 events\n",
      "Not setting metadata\n",
      "17385 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 17385 events and 101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Epochs shape: (17385, 8, 101)\n",
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shivani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 4 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (17385, 136)\n",
      "[4/7] Loading preprocessing artifacts...\n",
      "✓ Scaler loaded successfully (expecting 992 features)\n",
      "✓ Label encoder loaded successfully\n",
      "[5/7] Applying transformations...\n",
      "\n",
      "!!! Pipeline Failed !!!\n",
      "Error: Scaling failed: X has 1054 features, but StandardScaler is expecting 992 features as input.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shivani\\AppData\\Local\\Temp\\ipykernel_7468\\3810426759.py\", line 655, in run_pipeline\n",
      "    X_healthy = self.scaler.transform(X_healthy)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shivani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shivani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1062, in transform\n",
      "    X = validate_data(\n",
      "        ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shivani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 2965, in validate_data\n",
      "    _check_n_features(_estimator, X, reset=reset)\n",
      "  File \"C:\\Users\\shivani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 2829, in _check_n_features\n",
      "    raise ValueError(\n",
      "ValueError: X has 1054 features, but StandardScaler is expecting 992 features as input.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shivani\\AppData\\Local\\Temp\\ipykernel_7468\\3810426759.py\", line 660, in run_pipeline\n",
      "    raise ValueError(error_msg)\n",
      "ValueError: Scaling failed: X has 1054 features, but StandardScaler is expecting 992 features as input.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import mne\n",
    "import gc\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from scipy.signal import welch\n",
    "import pywt\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configure plotting\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "class EEGPipeline:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.scaler = None\n",
    "        self.label_encoder = None\n",
    "        self.common_channels = None\n",
    "        self.feature_names = [\n",
    "            'Delta Power', 'Theta Power', 'Alpha Power', 'Beta Power', 'Gamma Power',\n",
    "            'Wavelet Mean 1', 'Wavelet Mean 2', 'Wavelet Mean 3', 'Wavelet Mean 4', 'Wavelet Mean 5',\n",
    "            'Mean', 'Std Dev', 'Median', 'Skewness', 'Kurtosis', 'Hjorth Mobility', 'Hjorth Complexity'\n",
    "        ]\n",
    "        self.band_names = ['Delta', 'Theta', 'Alpha', 'Beta', 'Gamma']\n",
    "        self.channel_mapping = {}\n",
    "        self.expected_features_per_channel = 17  # Updated for new features\n",
    "        self._error_messages = []\n",
    "\n",
    "    def set_channel_mapping(self, mapping_dict):\n",
    "        \"\"\"Set manual channel name mapping between different naming conventions\"\"\"\n",
    "        self.channel_mapping = mapping_dict\n",
    "\n",
    "    def normalize_channel_name(self, channel_name):\n",
    "        \"\"\"Advanced channel name normalization with manual mapping support\"\"\"\n",
    "        if isinstance(channel_name, (list, np.ndarray)):\n",
    "            channel_name = channel_name[0]\n",
    "        channel_name = str(channel_name).strip().upper()\n",
    "        \n",
    "        if channel_name in self.channel_mapping:\n",
    "            return self.channel_mapping[channel_name]\n",
    "        \n",
    "        channel_name = re.sub(r'[^A-Z0-9]', '', channel_name)\n",
    "        channel_name = re.sub(r'^CH', '', channel_name)\n",
    "        channel_name = re.sub(r'^EEG', '', channel_name)\n",
    "        channel_name = channel_name.lstrip('0')\n",
    "        \n",
    "        variations = {\n",
    "            'FP1': 'Fp1', 'FP2': 'Fp2',\n",
    "            'T3': 'T7', 'T4': 'T8',\n",
    "            'T5': 'P7', 'T6': 'P8'\n",
    "        }\n",
    "        return variations.get(channel_name, channel_name)\n",
    "\n",
    "    def get_channel_names_from_mat(self, mat_path):\n",
    "        \"\"\"Robust MAT file channel extraction supporting multiple formats\"\"\"\n",
    "        try:\n",
    "            mat_data = loadmat(mat_path)\n",
    "            channels = []\n",
    "            \n",
    "            if 'data' in mat_data:\n",
    "                data_struct = mat_data['data'][0][0]\n",
    "                if 'channels' in data_struct.dtype.names:\n",
    "                    channels = [str(ch[0]) for ch in data_struct['channels'][0]]\n",
    "                elif 'chanlocs' in data_struct.dtype.names:\n",
    "                    chanlocs = data_struct['chanlocs'][0]\n",
    "                    channels = [str(chan['labels'][0]) for chan in chanlocs]\n",
    "            \n",
    "            elif 'EEG' in mat_data:\n",
    "                eeg_struct = mat_data['EEG'][0][0]\n",
    "                if 'chanlocs' in eeg_struct.dtype.names:\n",
    "                    chanlocs = eeg_struct['chanlocs'][0]\n",
    "                    channels = [str(chan['labels'][0]) for chan in chanlocs]\n",
    "                elif 'chaninfo' in eeg_struct.dtype.names:\n",
    "                    chaninfo = eeg_struct['chaninfo'][0][0]\n",
    "                    if 'labels' in chaninfo.dtype.names:\n",
    "                        channels = [str(ch[0]) for ch in chaninfo['labels'][0]]\n",
    "            \n",
    "            elif 'X' in mat_data and 'ch_names' in mat_data:\n",
    "                channels = [str(ch[0]) for ch in mat_data['ch_names'][0]]\n",
    "                \n",
    "            return [self.normalize_channel_name(ch) for ch in channels if ch and str(ch).strip()]\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading {mat_path}: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return []\n",
    "\n",
    "    def get_channel_names_from_vhdr(self, vhdr_path):\n",
    "        \"\"\"Extract channel names from BrainVision files with validation\"\"\"\n",
    "        try:\n",
    "            raw = mne.io.read_raw_brainvision(vhdr_path, preload=False, verbose=False)\n",
    "            return [self.normalize_channel_name(ch) for ch in raw.ch_names]\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading {vhdr_path}: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return []\n",
    "\n",
    "    def find_common_channels(self, healthy_channels, patient_channels):\n",
    "        \"\"\"Flexible channel matching with multiple strategies\"\"\"\n",
    "        common = set(healthy_channels).intersection(patient_channels)\n",
    "        \n",
    "        if not common:\n",
    "            healthy_set = set(healthy_channels)\n",
    "            patient_set = set(patient_channels)\n",
    "            common = healthy_set.intersection(patient_set)\n",
    "            \n",
    "        if not common:\n",
    "            common_partial = set()\n",
    "            for h_ch in healthy_set:\n",
    "                for p_ch in patient_set:\n",
    "                    if h_ch in p_ch or p_ch in h_ch:\n",
    "                        common_partial.add(h_ch)\n",
    "            if common_partial:\n",
    "                print(f\"Using partial channel matches: {common_partial}\")\n",
    "                return sorted(common_partial)\n",
    "            \n",
    "        print(f\"Found {len(common)} common channels\")\n",
    "        return sorted(common)\n",
    "\n",
    "    def collect_channel_names(self, folder_path, is_patient=False):\n",
    "        \"\"\"Collect channel names with extensive validation\"\"\"\n",
    "        all_channels = set()\n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith('.vhdr' if not is_patient else '.mat')]\n",
    "        \n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No valid files found in {folder_path}\")\n",
    "            \n",
    "        for file in tqdm(files, desc=f\"Collecting {'patient' if is_patient else 'healthy'} channels\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            channels = self.get_channel_names_from_mat(file_path) if is_patient else self.get_channel_names_from_vhdr(file_path)\n",
    "            \n",
    "            if not channels:\n",
    "                print(f\"Warning: No channels found in {file}\")\n",
    "                continue\n",
    "                \n",
    "            all_channels.update(channels)\n",
    "            \n",
    "        if not all_channels:\n",
    "            raise ValueError(f\"No channels collected from {folder_path}\")\n",
    "            \n",
    "        return sorted(all_channels)\n",
    "\n",
    "    def load_dataset(self, folder_path, is_patient=False, max_files=None, max_duration=None):\n",
    "        \"\"\"Robust dataset loading with comprehensive validation\"\"\"\n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith('.mat' if is_patient else '.vhdr')]\n",
    "        loader_func = self.load_mat_data if is_patient else self.load_brainvision_data\n",
    "            \n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No valid files found in {folder_path}\")\n",
    "            \n",
    "        if max_files:\n",
    "            files = files[:max_files]\n",
    "            \n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        sfreqs = []\n",
    "        loaded_files = 0\n",
    "        \n",
    "        for file in tqdm(files, desc=f\"Loading {'patient' if is_patient else 'healthy'} files\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            try:\n",
    "                data, labels, sfreq = loader_func(file_path, max_duration)\n",
    "                \n",
    "                if len(data) > 0 and len(labels) > 0:\n",
    "                    all_data.append(data)\n",
    "                    all_labels.append(labels)\n",
    "                    sfreqs.append(sfreq)\n",
    "                    loaded_files += 1\n",
    "                else:\n",
    "                    print(f\"Skipping {file} - no valid data\")\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error loading {file}: {str(e)}\"\n",
    "                self._error_messages.append(error_msg)\n",
    "                print(error_msg)\n",
    "                \n",
    "            gc.collect()\n",
    "            \n",
    "        print(f\"\\nSuccessfully loaded {loaded_files}/{len(files)} files\")\n",
    "        \n",
    "        if not all_data:\n",
    "            raise ValueError(\"No valid data loaded - check file formats\")\n",
    "            \n",
    "        avg_sfreq = np.mean(sfreqs) if sfreqs else (100 if is_patient else 250)\n",
    "        return np.concatenate(all_data), np.concatenate(all_labels), avg_sfreq\n",
    "\n",
    "    def load_brainvision_data(self, vhdr_path, max_duration=None):\n",
    "        \"\"\"Load BrainVision data with enhanced validation\"\"\"\n",
    "        try:\n",
    "            raw = mne.io.read_raw_brainvision(vhdr_path, preload=True, verbose=False)\n",
    "            \n",
    "            if max_duration:\n",
    "                crop_end = min(max_duration, raw.times[-1])\n",
    "                raw.crop(tmax=crop_end)\n",
    "                \n",
    "            # Apply standard preprocessing to match patient data\n",
    "            raw.filter(0.5, 45, fir_design='firwin', phase='zero-double')\n",
    "            raw.notch_filter(np.arange(50, 251, 50), filter_length='auto', phase='zero')\n",
    "            \n",
    "            data = raw.get_data().T.astype(np.float32)\n",
    "            events, _ = mne.events_from_annotations(raw)\n",
    "            labels = events[:, 2] if len(events) > 0 else np.zeros(len(data))\n",
    "            \n",
    "            return data, labels, raw.info['sfreq']\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading {vhdr_path}: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return np.array([]), np.array([]), None\n",
    "\n",
    "    def load_mat_data(self, mat_path, max_duration=None, default_sfreq=100):\n",
    "        \"\"\"Flexible MAT file loader supporting multiple structures\"\"\"\n",
    "        try:\n",
    "            mat_data = loadmat(mat_path)\n",
    "            eeg_data = None\n",
    "            labels = None\n",
    "            sfreq = default_sfreq\n",
    "            \n",
    "            if 'data' in mat_data:\n",
    "                data_struct = mat_data['data'][0][0]\n",
    "                if 'X' in data_struct.dtype.names:\n",
    "                    eeg_data = data_struct['X']\n",
    "                    if eeg_data.ndim > 2:\n",
    "                        eeg_data = eeg_data.reshape(eeg_data.shape[0], -1)\n",
    "                    if 'y' in data_struct.dtype.names:\n",
    "                        labels = data_struct['y'].flatten()\n",
    "                    if 'sfreq' in data_struct.dtype.names:\n",
    "                        sfreq = float(data_struct['sfreq'][0][0])\n",
    "                    elif 'Fs' in data_struct.dtype.names:\n",
    "                        sfreq = float(data_struct['Fs'][0][0])\n",
    "            \n",
    "            elif 'EEG' in mat_data:\n",
    "                eeg_struct = mat_data['EEG'][0][0]\n",
    "                if 'data' in eeg_struct.dtype.names:\n",
    "                    eeg_data = eeg_struct['data'].T\n",
    "                if 'event' in eeg_struct.dtype.names:\n",
    "                    events = eeg_struct['event'][0]\n",
    "                    labels = np.array([ev[0]['type'][0] for ev in events])\n",
    "                if 'srate' in eeg_struct.dtype.names:\n",
    "                    sfreq = float(eeg_struct['srate'][0][0])\n",
    "            \n",
    "            elif 'X' in mat_data:\n",
    "                eeg_data = mat_data['X']\n",
    "                if 'y' in mat_data:\n",
    "                    labels = mat_data['y'].flatten()\n",
    "            \n",
    "            if labels is None or len(np.unique(labels)) <= 1:\n",
    "                labels = np.zeros(len(eeg_data)) if eeg_data is not None else np.array([])\n",
    "            \n",
    "            if eeg_data is None:\n",
    "                raise ValueError(\"No EEG data found in MAT file\")\n",
    "            \n",
    "            min_len = min(len(eeg_data), len(labels))\n",
    "            eeg_data = eeg_data[:min_len]\n",
    "            labels = labels[:min_len]\n",
    "            \n",
    "            if max_duration:\n",
    "                max_samples = int(max_duration * sfreq)\n",
    "                eeg_data = eeg_data[:max_samples]\n",
    "                labels = labels[:max_samples]\n",
    "                \n",
    "            return eeg_data.astype(np.float32), labels, sfreq\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading {mat_path}: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return np.array([]), np.array([]), None\n",
    "\n",
    "    def hjorth_parameters(self, signal):\n",
    "        \"\"\"Calculate Hjorth mobility and complexity parameters\"\"\"\n",
    "        first_deriv = np.diff(signal)\n",
    "        second_deriv = np.diff(signal, 2)\n",
    "        \n",
    "        var_zero = np.var(signal)\n",
    "        var_d1 = np.var(first_deriv)\n",
    "        var_d2 = np.var(second_deriv)\n",
    "        \n",
    "        mobility = np.sqrt(var_d1 / var_zero)\n",
    "        complexity = np.sqrt(var_d2 / var_d1) / mobility\n",
    "        \n",
    "        return mobility, complexity\n",
    "\n",
    "    def preprocess_data(self, data, labels, sfreq=250, common_channels=None):\n",
    "        \"\"\"Enhanced preprocessing with detailed debugging\"\"\"\n",
    "        print(\"\\n=== Starting Preprocessing ===\")\n",
    "        print(f\"Input data shape: {data.shape if data is not None else 'None'}\")\n",
    "        print(f\"Sample rate: {sfreq} Hz\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Validate input data\n",
    "            if data is None or len(data) == 0:\n",
    "                raise ValueError(\"Empty data array received\")\n",
    "                \n",
    "            if len(data.shape) != 2:\n",
    "                raise ValueError(f\"Expected 2D array, got {len(data.shape)}D array\")\n",
    "                \n",
    "            n_channels = data.shape[1]\n",
    "            print(f\"Number of channels: {n_channels}\")\n",
    "            \n",
    "            if n_channels == 0:\n",
    "                raise ValueError(\"No channels found in data\")\n",
    "    \n",
    "            # 2. Create RawArray\n",
    "            ch_names = common_channels[:n_channels] if common_channels else [f'ch{i}' for i in range(n_channels)]\n",
    "            print(f\"First 3 channel names: {ch_names[:3]}\")\n",
    "            \n",
    "            info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')\n",
    "            print(\"Created info structure\")\n",
    "            \n",
    "            try:\n",
    "                raw = mne.io.RawArray(data.T, info)\n",
    "                print(\"Successfully created RawArray\")\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Failed to create RawArray: {str(e)}\")\n",
    "    \n",
    "            # 3. Apply filters\n",
    "            nyquist = sfreq / 2\n",
    "            print(f\"Nyquist frequency: {nyquist} Hz\")\n",
    "            \n",
    "            try:\n",
    "                print(\"Applying bandpass filter...\")\n",
    "                raw.filter(0.5, min(45, nyquist-1), fir_design='firwin', phase='zero-double')\n",
    "                \n",
    "                print(\"Applying notch filter...\")\n",
    "                notch_freqs = [f for f in np.arange(50, 251, 50) if f < nyquist]\n",
    "                if notch_freqs:\n",
    "                    raw.notch_filter(notch_freqs, filter_length='auto', phase='zero')\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Filtering failed: {str(e)}\")\n",
    "    \n",
    "            # 4. Create epochs\n",
    "            try:\n",
    "                print(\"Creating epochs...\")\n",
    "                events = mne.make_fixed_length_events(raw, duration=1.0)\n",
    "                print(f\"Created {len(events)} events\")\n",
    "                \n",
    "                epochs = mne.Epochs(raw, events, tmin=0, tmax=1.0, baseline=None, preload=True)\n",
    "                epochs_data = epochs.get_data()\n",
    "                print(f\"Epochs shape: {epochs_data.shape}\")\n",
    "                \n",
    "                if len(epochs_data) == 0:\n",
    "                    raise ValueError(\"No epochs created - check duration parameters\")\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Epoch creation failed: {str(e)}\")\n",
    "    \n",
    "            # 5. Feature extraction\n",
    "            print(\"Extracting features...\")\n",
    "            X, y = self._extract_features(epochs_data, labels[:len(epochs_data)], sfreq, nyquist)\n",
    "            print(f\"Extracted features shape: {X.shape}\")\n",
    "            \n",
    "            if len(X) == 0:\n",
    "                raise ValueError(\"Feature extraction returned empty array\")\n",
    "                \n",
    "            return X, y\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Preprocessing failed: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            self._error_messages.append(error_msg)\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "    def _extract_features(self, epochs_data, labels, sfreq, nyquist):\n",
    "            \"\"\"Internal feature extraction method\"\"\"\n",
    "            features = []\n",
    "            n_channels = epochs_data.shape[1]\n",
    "            \n",
    "            for epoch in epochs_data:\n",
    "                epoch_features = []\n",
    "                for channel in epoch:\n",
    "                    # Frequency features\n",
    "                    try:\n",
    "                        freqs, psd = welch(channel, fs=sfreq, nperseg=min(256, len(channel)))\n",
    "                        bands = {'delta': (0.5, 4), 'theta': (4, 8), 'alpha': (8, 12),\n",
    "                                'beta': (12, 30), 'gamma': (30, min(45, nyquist-1))}\n",
    "                        band_powers = [np.sum(psd[(freqs >= low) & (freqs <= high)]) \n",
    "                                     for low, high in bands.values()]\n",
    "                    except:\n",
    "                        band_powers = [0.0] * len(self.band_names)\n",
    "        \n",
    "                    # Wavelet features\n",
    "                    try:\n",
    "                        coeffs = pywt.wavedec(channel, 'db4', level=4)\n",
    "                        wavelet_features = [np.mean(c) for c in coeffs[:5]]\n",
    "                        if len(wavelet_features) < 5:\n",
    "                            wavelet_features += [0.0] * (5 - len(wavelet_features))\n",
    "                    except:\n",
    "                        wavelet_features = [0.0] * 5\n",
    "        \n",
    "                    # Statistical features\n",
    "                    stats = [\n",
    "                        np.mean(channel),\n",
    "                        np.std(channel),\n",
    "                        np.median(channel),\n",
    "                        pd.Series(channel).skew(),\n",
    "                        pd.Series(channel).kurtosis()\n",
    "                    ]\n",
    "                    \n",
    "                    # Hjorth parameters\n",
    "                    try:\n",
    "                        mobility, complexity = self.hjorth_parameters(channel)\n",
    "                        hjorth_features = [mobility, complexity]\n",
    "                    except:\n",
    "                        hjorth_features = [0.0, 0.0]\n",
    "        \n",
    "                    epoch_features.extend(band_powers + wavelet_features + stats + hjorth_features)\n",
    "                \n",
    "                features.append(epoch_features)\n",
    "            \n",
    "            X = np.array(features)\n",
    "            y = np.array(labels)\n",
    "            \n",
    "            # Handle empty case\n",
    "            if len(X) == 0:\n",
    "                return np.empty((0, self.expected_features_per_channel * n_channels)), np.array([])\n",
    "            \n",
    "            return X, y\n",
    "\n",
    "    def load_preprocessing_artifacts(self, scaler_path, label_encoder_path):\n",
    "        \"\"\"Load preprocessing artifacts with dimension validation\"\"\"\n",
    "        try:\n",
    "            self.scaler = joblib.load(scaler_path)\n",
    "            print(f\"✓ Scaler loaded successfully (expecting {self.scaler.n_features_in_} features)\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"✗ Failed to load scaler: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            self.scaler = None\n",
    "            \n",
    "        try:\n",
    "            self.label_encoder = joblib.load(label_encoder_path)\n",
    "            print(\"✓ Label encoder loaded successfully\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"✗ Failed to load label encoder: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            self.label_encoder = None\n",
    "\n",
    "    def match_features(self, X, expected_features):\n",
    "        \"\"\"Ensure feature matrix matches expected dimensions\"\"\"\n",
    "        if X is None or len(X) == 0:\n",
    "            return np.array([])\n",
    "            \n",
    "        if X.shape[1] == expected_features:\n",
    "            return X\n",
    "        elif X.shape[1] < expected_features:\n",
    "            pad_width = ((0, 0), (0, expected_features - X.shape[1]))\n",
    "            return np.pad(X, pad_width, mode='constant')\n",
    "        else:\n",
    "            return X[:, :expected_features]\n",
    "\n",
    "    def load_models(self, model_paths):\n",
    "        \"\"\"Load models with comprehensive validation\"\"\"\n",
    "        self.models = {}\n",
    "        for name, path in model_paths.items():\n",
    "            try:\n",
    "                self.models[name] = joblib.load(path)\n",
    "                print(f\"✓ {name} model loaded successfully\")\n",
    "            except Exception as e:\n",
    "                error_msg = f\"✗ Failed to load {name} model: {str(e)}\"\n",
    "                self._error_messages.append(error_msg)\n",
    "                print(error_msg)\n",
    "                \n",
    "        if not self.models:\n",
    "            raise ValueError(\"No models were loaded successfully\")\n",
    "        return self.models\n",
    "\n",
    "    def evaluate_models(self, X_data, y_data, data_type='Patient'):\n",
    "        \"\"\"Enhanced evaluation with better ROC curve handling\"\"\"\n",
    "        self.results[data_type] = {}\n",
    "        for name, model in self.models.items():\n",
    "            try:\n",
    "                y_pred = model.predict(X_data)\n",
    "                y_proba = model.predict_proba(X_data)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "                \n",
    "                acc = accuracy_score(y_data, y_pred)\n",
    "                report = classification_report(y_data, y_pred, output_dict=True, zero_division=0)\n",
    "                cm = confusion_matrix(y_data, y_pred)\n",
    "                \n",
    "                # ROC curve calculation\n",
    "                roc_auc, fpr, tpr = None, None, None\n",
    "                if y_proba is not None and len(np.unique(y_data)) == 2:\n",
    "                    fpr, tpr, _ = roc_curve(y_data, y_proba)\n",
    "                    roc_auc = auc(fpr, tpr)\n",
    "                \n",
    "                # Precision-Recall curve\n",
    "                pr_auc, precision, recall = None, None, None\n",
    "                if y_proba is not None and len(np.unique(y_data)) == 2:\n",
    "                    precision, recall, _ = precision_recall_curve(y_data, y_proba)\n",
    "                    pr_auc = average_precision_score(y_data, y_proba)\n",
    "                \n",
    "                self.results[data_type][name] = {\n",
    "                    'accuracy': acc,\n",
    "                    'report': report,\n",
    "                    'confusion_matrix': cm,\n",
    "                    'roc_auc': roc_auc,\n",
    "                    'fpr': fpr,\n",
    "                    'tpr': tpr,\n",
    "                    'pr_auc': pr_auc,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'y_true': y_data,\n",
    "                    'y_pred': y_pred,\n",
    "                    'y_proba': y_proba\n",
    "                }\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error evaluating {name} on {data_type} data: {str(e)}\"\n",
    "                self._error_messages.append(error_msg)\n",
    "                print(error_msg)\n",
    "\n",
    "    def plot_eeg_comparison(self, healthy_data, patient_data, healthy_sfreq=250, patient_sfreq=100, samples=500, channels=3):\n",
    "        \"\"\"Enhanced EEG signal comparison\"\"\"\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), sharey=True)\n",
    "        \n",
    "        time_h = np.arange(min(samples, len(healthy_data))) / healthy_sfreq\n",
    "        time_p = np.arange(min(samples, len(patient_data))) / patient_sfreq\n",
    "        \n",
    "        # Healthy EEG\n",
    "        for ch in range(min(channels, healthy_data.shape[1])):\n",
    "            ax1.plot(time_h, healthy_data[:len(time_h), ch] * 1e6,\n",
    "                    linewidth=1.5, alpha=0.8, label=f'Ch{ch+1}')\n",
    "        \n",
    "        ax1.set_title('Healthy EEG', fontsize=16, pad=20)\n",
    "        ax1.set_xlabel('Time (s)', fontsize=14)\n",
    "        ax1.set_ylabel('Amplitude (μV)', fontsize=14)\n",
    "        ax1.legend(loc='upper right', fontsize=10)\n",
    "        ax1.grid(True, linestyle=':', alpha=0.5)\n",
    "        ax1.set_ylim(-150, 150)\n",
    "        \n",
    "        # Patient EEG\n",
    "        for ch in range(min(channels, patient_data.shape[1])):\n",
    "            ax2.plot(time_p, patient_data[:len(time_p), ch] * 1e6,\n",
    "                    linewidth=1.5, alpha=0.8, label=f'Ch{ch+1}')\n",
    "        \n",
    "        ax2.set_title('Patient EEG', fontsize=16, pad=20)\n",
    "        ax2.set_xlabel('Time (s)', fontsize=14)\n",
    "        ax2.legend(loc='upper right', fontsize=10)\n",
    "        ax2.grid(True, linestyle=':', alpha=0.5)\n",
    "        ax2.set_ylim(-150, 150)\n",
    "        \n",
    "        plt.suptitle('EEG Signal Comparison', fontsize=18, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_roc_curves(self):\n",
    "        \"\"\"Enhanced ROC curve plotting with AUC values\"\"\"\n",
    "        if not self.results or 'Patient' not in self.results:\n",
    "            print(\"No patient results available for ROC curves\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for model_name, results in self.results['Patient'].items():\n",
    "            if results.get('roc_auc') is not None:\n",
    "                plt.plot(results['fpr'], results['tpr'],\n",
    "                        lw=2, label=f'{model_name} (AUC = {results[\"roc_auc\"]:.2f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate', fontsize=12)\n",
    "        plt.ylabel('True Positive Rate', fontsize=12)\n",
    "        plt.title('Receiver Operating Characteristic', fontsize=16)\n",
    "        plt.legend(loc=\"lower right\", fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_precision_recall_curves(self):\n",
    "        \"\"\"Plot precision-recall curves\"\"\"\n",
    "        if not self.results or 'Patient' not in self.results:\n",
    "            print(\"No patient results available for PR curves\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for model_name, results in self.results['Patient'].items():\n",
    "            if results.get('pr_auc') is not None:\n",
    "                plt.plot(results['recall'], results['precision'],\n",
    "                        lw=2, label=f'{model_name} (AP = {results[\"pr_auc\"]:.2f})')\n",
    "        \n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Recall', fontsize=12)\n",
    "        plt.ylabel('Precision', fontsize=12)\n",
    "        plt.title('Precision-Recall Curve', fontsize=16)\n",
    "        plt.legend(loc=\"upper right\", fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    def run_pipeline(self, config):\n",
    "        \"\"\"Enhanced pipeline with better error handling\"\"\"\n",
    "        results = None\n",
    "        try:\n",
    "            print(\"\\n=== EEG Analysis Pipeline ===\\n\")\n",
    "            \n",
    "            # 1. Load data\n",
    "            print(\"[1/7] Loading data...\")\n",
    "            healthy_data, healthy_labels, h_sfreq = self.load_dataset(\n",
    "                config['healthy_path'],\n",
    "                max_files=config.get('max_files'),\n",
    "                max_duration=config.get('max_duration', None))\n",
    "            patient_data, patient_labels, p_sfreq = self.load_dataset(\n",
    "                config['patient_path'],\n",
    "                is_patient=True,\n",
    "                max_files=config.get('max_files'),\n",
    "                max_duration=config.get('max_duration', None))\n",
    "            \n",
    "            # 2. Channel matching\n",
    "            print(\"[2/7] Finding common channels...\")\n",
    "            if not config.get('skip_channel_matching', False):\n",
    "                healthy_channels = self.collect_channel_names(config['healthy_path'])\n",
    "                patient_channels = self.collect_channel_names(config['patient_path'], is_patient=True)\n",
    "                self.common_channels = self.find_common_channels(healthy_channels, patient_channels)\n",
    "            \n",
    "            # 3. Preprocessing\n",
    "            print(\"[3/7] Preprocessing data...\")\n",
    "            X_healthy, y_healthy = self.preprocess_data(\n",
    "                healthy_data, healthy_labels,\n",
    "                sfreq=h_sfreq,\n",
    "                common_channels=self.common_channels)\n",
    "            X_patients, y_patients = self.preprocess_data(\n",
    "                patient_data, patient_labels,\n",
    "                sfreq=p_sfreq,\n",
    "                common_channels=self.common_channels)\n",
    "            \n",
    "            # 4. Load artifacts\n",
    "            print(\"[4/7] Loading preprocessing artifacts...\")\n",
    "            self.load_preprocessing_artifacts(\n",
    "                config['scaler_path'],\n",
    "                config['label_encoder_path'])\n",
    "            \n",
    "            # 5. Apply transformations\n",
    "            print(\"[5/7] Applying transformations...\")\n",
    "            if self.scaler:\n",
    "                # Validate data before transformation\n",
    "                if len(X_healthy) == 0 or len(X_patients) == 0:\n",
    "                    raise ValueError(\"Empty feature arrays - check preprocessing output\")\n",
    "                \n",
    "                try:\n",
    "                    X_healthy = self.scaler.transform(X_healthy)\n",
    "                    X_patients = self.scaler.transform(X_patients)\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Scaling failed: {str(e)}\"\n",
    "                    self._error_messages.append(error_msg)\n",
    "                    raise ValueError(error_msg)\n",
    "            \n",
    "            if self.label_encoder:\n",
    "                try:\n",
    "                    y_patients = self.label_encoder.transform(y_patients)\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Label encoding failed: {str(e)}\"\n",
    "                    self._error_messages.append(error_msg)\n",
    "                    raise ValueError(error_msg)\n",
    "            \n",
    "            # 6. Load models\n",
    "            print(\"[6/7] Loading models...\")\n",
    "            self.load_models(config['model_paths'])\n",
    "            \n",
    "            # 7. Evaluate models\n",
    "            print(\"[7/7] Evaluating models...\")\n",
    "            self.evaluate_models(X_patients, y_patients, data_type='Patient')\n",
    "            \n",
    "            # Generate visualizations\n",
    "            print(\"\\n=== Generating Visualizations ===\\n\")\n",
    "            try:\n",
    "                plt.style.use('seaborn-v0_8-whitegrid')\n",
    "            except:\n",
    "                plt.style.use('ggplot')\n",
    "            \n",
    "            vis_funcs = [\n",
    "                ('EEG Comparison', self.plot_eeg_comparison, [healthy_data, patient_data, h_sfreq, p_sfreq]),\n",
    "                ('ROC Curves', self.plot_roc_curves, []),\n",
    "                ('PR Curves', self.plot_precision_recall_curves, [])\n",
    "            ]\n",
    "            \n",
    "            for name, func, args in vis_funcs:\n",
    "                try:\n",
    "                    print(f\"Generating {name} visualization...\")\n",
    "                    func(*args)\n",
    "                    plt.close('all')\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Failed to generate {name}: {str(e)}\"\n",
    "                    self._error_messages.append(error_msg)\n",
    "                    print(error_msg)\n",
    "            \n",
    "            print(\"\\n=== Pipeline Completed ===\\n\")\n",
    "            if self._error_messages:\n",
    "                print(\"Completed with warnings/errors (see above messages)\")\n",
    "            else:\n",
    "                print(\"Completed successfully!\")\n",
    "            \n",
    "            results = {\n",
    "                'healthy_data': (X_healthy, y_healthy),\n",
    "                'patient_data': (X_patients, y_patients),\n",
    "                'results': self.results,\n",
    "                'metadata': {\n",
    "                    'healthy_samples': len(X_healthy),\n",
    "                    'patient_samples': len(X_patients),\n",
    "                    'common_channels': self.common_channels,\n",
    "                    'features_per_channel': self.expected_features_per_channel,\n",
    "                    'errors': self._error_messages\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n!!! Pipeline Failed !!!\\nError: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "        finally:\n",
    "            if 'healthy_data' in locals():\n",
    "                del healthy_data\n",
    "            if 'patient_data' in locals():\n",
    "                del patient_data\n",
    "            gc.collect()\n",
    "            \n",
    "        return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    channel_mapping = {\n",
    "        'FP1': 'Fp1',\n",
    "        'FP2': 'Fp2',\n",
    "        'F3': 'F3',\n",
    "        'F4': 'F4',\n",
    "    }\n",
    "\n",
    "    config = {\n",
    "        'healthy_path': \"F:/shivani/VSCode/ml/worked on dataset/3(final)/Healthy\",\n",
    "        'patient_path': \"F:/shivani/VSCode/ml/worked on dataset/4/dataset/Patients\",\n",
    "        'model_paths': {\n",
    "            \"SVM\": \"F:/shivani/VSCode/ml/worked on dataset/3(final)/svm_model.pkl\",\n",
    "            \"RF\": \"F:/shivani/VSCode/ml/worked on dataset/3(final)/rf_model.pkl\",\n",
    "            \"XGB\": \"F:/shivani/VSCode/ml/worked on dataset/3(final)/xgb_model.pkl\"\n",
    "        },\n",
    "        'scaler_path': \"F:/shivani/VSCode/ml/worked on dataset/3(final)/preprocessing_artifacts/eeg_scaler.joblib\",\n",
    "        'label_encoder_path': \"F:/shivani/VSCode/ml/worked on dataset/3(final)/preprocessing_artifacts/label_encoder.joblib\",\n",
    "        'max_files': 5,\n",
    "        'skip_channel_matching': True  # Skip dynamic channel matching\n",
    "    }\n",
    "\n",
    "    pipeline = EEGPipeline()\n",
    "    pipeline.set_channel_mapping(channel_mapping)\n",
    "    results = pipeline.run_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EEG Analysis Pipeline ===\n",
      "\n",
      "\n",
      "=== Data Validation ===\n",
      "Healthy files:\n",
      "Found 40 files\n",
      "Sample healthy file: sub-010002 - Copy.vhdr\n",
      "\n",
      "Patient files:\n",
      "Found 8 files\n",
      "Sample patient file: A01.mat\n",
      "[1/7] Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading healthy files:   0%|                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-12 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    5.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    3.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['Comment/no USB Connection to actiCAP', 'New Segment/', 'Stimulus/S  1', 'Stimulus/S200', 'Stimulus/S210']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading healthy files:  20%|████████████▏                                                | 1/5 [00:37<02:30, 37.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-12 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    5.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    3.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['Comment/no USB Connection to actiCAP', 'New Segment/', 'Stimulus/S  1', 'Stimulus/S200', 'Stimulus/S210']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading healthy files:  40%|████████████████████████▍                                    | 2/5 [01:14<01:51, 37.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-12 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    5.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    2.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['Comment/no USB Connection to actiCAP', 'New Segment/', 'Stimulus/S  1', 'Stimulus/S200', 'Stimulus/S210']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading healthy files:  60%|████████████████████████████████████▌                        | 3/5 [01:51<01:14, 37.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-12 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    5.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    3.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['Comment/no USB Connection to actiCAP', 'New Segment/', 'Stimulus/S  1', 'Stimulus/S200', 'Stimulus/S210']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading healthy files:  80%|████████████████████████████████████████████████▊            | 4/5 [02:28<00:37, 37.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-12 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    5.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 16501 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    2.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['Comment/actiCAP Data On', 'New Segment/', 'Stimulus/S  1', 'Stimulus/S200', 'Stimulus/S210']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading healthy files: 100%|█████████████████████████████████████████████████████████████| 5/5 [03:05<00:00, 37.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded 5/5 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading patient files: 100%|█████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded 5/5 files\n",
      "[2/7] Finding common channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting healthy channels:  30%|███████████████▉                                     | 12/40 [00:00<00:01, 16.00it/s]C:\\Users\\shivani\\AppData\\Local\\Temp\\ipykernel_7468\\1448636875.py:114: RuntimeWarning: Omitted 313 annotation(s) that were outside data range.\n",
      "  raw = mne.io.read_raw_brainvision(vhdr_path, preload=False, verbose=False)\n",
      "Collecting healthy channels:  50%|██████████████████████████▌                          | 20/40 [00:01<00:00, 20.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading F:/shivani/VSCode/ml/worked on dataset/3(final)/Healthy\\sub-010020.vhdr: [Errno 2] No such file or directory: 'F:\\\\shivani\\\\VSCode\\\\ml\\\\worked on dataset\\\\3(final)\\\\Healthy\\\\Untitled.vmrk'\n",
      "Warning: No channels found in sub-010020.vhdr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting healthy channels: 100%|█████████████████████████████████████████████████████| 40/40 [00:02<00:00, 17.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading F:/shivani/VSCode/ml/worked on dataset/3(final)/Healthy\\sub-010044.vhdr: [Errno 2] No such file or directory: 'F:\\\\shivani\\\\VSCode\\\\ml\\\\worked on dataset\\\\3(final)\\\\Healthy\\\\sub-010044eeg.vmrk'\n",
      "Warning: No channels found in sub-010044.vhdr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting patient channels: 100%|███████████████████████████████████████████████████████| 8/8 [00:02<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 common channels\n",
      "[3/7] Preprocessing data...\n",
      "\n",
      "=== Starting Preprocessing ===\n",
      "Input data shape: (12704800, 62)\n",
      "Sample rate: 2500.0 Hz\n",
      "Number of channels: 62\n",
      "First 3 channel names: ['CZ', 'FZ', 'OZ']\n",
      "Created info structure\n",
      "Preprocessing failed: Failed to create RawArray: len(data) (62) does not match len(info[\"ch_names\"]) (8)\n",
      "\n",
      "=== Starting Preprocessing ===\n",
      "Input data shape: (1738520, 8)\n",
      "Sample rate: 100.0 Hz\n",
      "Number of channels: 8\n",
      "First 3 channel names: ['CZ', 'FZ', 'OZ']\n",
      "Created info structure\n",
      "Creating RawArray with float64 data, n_channels=8, n_times=1738520\n",
      "    Range : 0 ... 1738519 =      0.000 ... 17385.190 secs\n",
      "Ready.\n",
      "Successfully created RawArray\n",
      "Nyquist frequency: 50.0 Hz\n",
      "Applying bandpass filter...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 5.00 Hz (-12 dB cutoff frequency: 47.50 Hz)\n",
      "- Filter length: 661 samples (6.610 s)\n",
      "\n",
      "Applying notch filter...\n",
      "Creating epochs...\n",
      "Created 17385 events\n",
      "Not setting metadata\n",
      "17385 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 17385 events and 101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Epochs shape: (17385, 8, 101)\n",
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shivani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 4 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (17385, 136)\n",
      "[4/7] Loading preprocessing artifacts...\n",
      "✓ Scaler loaded successfully (expecting 992 features)\n",
      "✓ Label encoder loaded successfully\n",
      "[5/7] Applying transformations...\n",
      "\n",
      "!!! Pipeline Failed !!!\n",
      "Error: Empty feature arrays - check preprocessing output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shivani\\AppData\\Local\\Temp\\ipykernel_7468\\1448636875.py\", line 687, in run_pipeline\n",
      "    raise ValueError(\"Empty feature arrays - check preprocessing output\")\n",
      "ValueError: Empty feature arrays - check preprocessing output\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import mne\n",
    "import gc\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from scipy.signal import welch\n",
    "import pywt\n",
    "import pandas as pd\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Configure plotting\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "class EEGPipeline:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.scaler = None\n",
    "        self.label_encoder = None\n",
    "        self.common_channels = None\n",
    "        self.feature_names = [\n",
    "            'Delta Power', 'Theta Power', 'Alpha Power', 'Beta Power', 'Gamma Power',\n",
    "            'Wavelet Mean 1', 'Wavelet Mean 2', 'Wavelet Mean 3', 'Wavelet Mean 4', 'Wavelet Mean 5',\n",
    "            'Mean', 'Std Dev', 'Median', 'Skewness', 'Kurtosis', 'Hjorth Mobility', 'Hjorth Complexity'\n",
    "        ]\n",
    "        self.band_names = ['Delta', 'Theta', 'Alpha', 'Beta', 'Gamma']\n",
    "        self.channel_mapping = {}\n",
    "        self.expected_features_per_channel = 17  # Updated for new features\n",
    "        self._error_messages = []\n",
    "        self.pca = None\n",
    "        self.feature_selector = None\n",
    "\n",
    "    def set_channel_mapping(self, mapping_dict):\n",
    "        \"\"\"Set manual channel name mapping between different naming conventions\"\"\"\n",
    "        self.channel_mapping = mapping_dict\n",
    "\n",
    "    def normalize_channel_name(self, channel_name):\n",
    "        \"\"\"Advanced channel name normalization with manual mapping support\"\"\"\n",
    "        if isinstance(channel_name, (list, np.ndarray)):\n",
    "            channel_name = channel_name[0]\n",
    "        channel_name = str(channel_name).strip().upper()\n",
    "        \n",
    "        if channel_name in self.channel_mapping:\n",
    "            return self.channel_mapping[channel_name]\n",
    "        \n",
    "        channel_name = re.sub(r'[^A-Z0-9]', '', channel_name)\n",
    "        channel_name = re.sub(r'^CH', '', channel_name)\n",
    "        channel_name = re.sub(r'^EEG', '', channel_name)\n",
    "        channel_name = channel_name.lstrip('0')\n",
    "        \n",
    "        variations = {\n",
    "            'FP1': 'Fp1', 'FP2': 'Fp2',\n",
    "            'T3': 'T7', 'T4': 'T8',\n",
    "            'T5': 'P7', 'T6': 'P8'\n",
    "        }\n",
    "        return variations.get(channel_name, channel_name)\n",
    "\n",
    "    def get_channel_names_from_mat(self, mat_path):\n",
    "        \"\"\"Robust MAT file channel extraction supporting multiple formats\"\"\"\n",
    "        try:\n",
    "            mat_data = loadmat(mat_path)\n",
    "            channels = []\n",
    "            \n",
    "            if 'data' in mat_data:\n",
    "                data_struct = mat_data['data'][0][0]\n",
    "                if 'channels' in data_struct.dtype.names:\n",
    "                    channels = [str(ch[0]) for ch in data_struct['channels'][0]]\n",
    "                elif 'chanlocs' in data_struct.dtype.names:\n",
    "                    chanlocs = data_struct['chanlocs'][0]\n",
    "                    channels = [str(chan['labels'][0]) for chan in chanlocs]\n",
    "            \n",
    "            elif 'EEG' in mat_data:\n",
    "                eeg_struct = mat_data['EEG'][0][0]\n",
    "                if 'chanlocs' in eeg_struct.dtype.names:\n",
    "                    chanlocs = eeg_struct['chanlocs'][0]\n",
    "                    channels = [str(chan['labels'][0]) for chan in chanlocs]\n",
    "                elif 'chaninfo' in eeg_struct.dtype.names:\n",
    "                    chaninfo = eeg_struct['chaninfo'][0][0]\n",
    "                    if 'labels' in chaninfo.dtype.names:\n",
    "                        channels = [str(ch[0]) for ch in chaninfo['labels'][0]]\n",
    "            \n",
    "            elif 'X' in mat_data and 'ch_names' in mat_data:\n",
    "                channels = [str(ch[0]) for ch in mat_data['ch_names'][0]]\n",
    "                \n",
    "            return [self.normalize_channel_name(ch) for ch in channels if ch and str(ch).strip()]\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading {mat_path}: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return []\n",
    "\n",
    "    def get_channel_names_from_vhdr(self, vhdr_path):\n",
    "        \"\"\"Extract channel names from BrainVision files with validation\"\"\"\n",
    "        try:\n",
    "            raw = mne.io.read_raw_brainvision(vhdr_path, preload=False, verbose=False)\n",
    "            return [self.normalize_channel_name(ch) for ch in raw.ch_names]\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading {vhdr_path}: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return []\n",
    "\n",
    "    def find_common_channels(self, healthy_channels, patient_channels):\n",
    "        \"\"\"Flexible channel matching with multiple strategies\"\"\"\n",
    "        common = set(healthy_channels).intersection(patient_channels)\n",
    "        \n",
    "        if not common:\n",
    "            healthy_set = set(healthy_channels)\n",
    "            patient_set = set(patient_channels)\n",
    "            common = healthy_set.intersection(patient_set)\n",
    "            \n",
    "        if not common:\n",
    "            common_partial = set()\n",
    "            for h_ch in healthy_set:\n",
    "                for p_ch in patient_set:\n",
    "                    if h_ch in p_ch or p_ch in h_ch:\n",
    "                        common_partial.add(h_ch)\n",
    "            if common_partial:\n",
    "                print(f\"Using partial channel matches: {common_partial}\")\n",
    "                return sorted(common_partial)\n",
    "            \n",
    "        print(f\"Found {len(common)} common channels\")\n",
    "        return sorted(common)\n",
    "\n",
    "    def collect_channel_names(self, folder_path, is_patient=False):\n",
    "        \"\"\"Collect channel names with extensive validation\"\"\"\n",
    "        all_channels = set()\n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith('.vhdr' if not is_patient else '.mat')]\n",
    "        \n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No valid files found in {folder_path}\")\n",
    "            \n",
    "        for file in tqdm(files, desc=f\"Collecting {'patient' if is_patient else 'healthy'} channels\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            channels = self.get_channel_names_from_mat(file_path) if is_patient else self.get_channel_names_from_vhdr(file_path)\n",
    "            \n",
    "            if not channels:\n",
    "                print(f\"Warning: No channels found in {file}\")\n",
    "                continue\n",
    "                \n",
    "            all_channels.update(channels)\n",
    "            \n",
    "        if not all_channels:\n",
    "            raise ValueError(f\"No channels collected from {folder_path}\")\n",
    "            \n",
    "        return sorted(all_channels)\n",
    "\n",
    "    def load_dataset(self, folder_path, is_patient=False, max_files=None, max_duration=None):\n",
    "        \"\"\"Robust dataset loading with comprehensive validation\"\"\"\n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith('.mat' if is_patient else '.vhdr')]\n",
    "        loader_func = self.load_mat_data if is_patient else self.load_brainvision_data\n",
    "            \n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No valid files found in {folder_path}\")\n",
    "            \n",
    "        if max_files:\n",
    "            files = files[:max_files]\n",
    "            \n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        sfreqs = []\n",
    "        loaded_files = 0\n",
    "        \n",
    "        for file in tqdm(files, desc=f\"Loading {'patient' if is_patient else 'healthy'} files\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            try:\n",
    "                data, labels, sfreq = loader_func(file_path, max_duration)\n",
    "                \n",
    "                if len(data) > 0 and len(labels) > 0:\n",
    "                    all_data.append(data)\n",
    "                    all_labels.append(labels)\n",
    "                    sfreqs.append(sfreq)\n",
    "                    loaded_files += 1\n",
    "                else:\n",
    "                    print(f\"Skipping {file} - no valid data\")\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error loading {file}: {str(e)}\"\n",
    "                self._error_messages.append(error_msg)\n",
    "                print(error_msg)\n",
    "                \n",
    "            gc.collect()\n",
    "            \n",
    "        print(f\"\\nSuccessfully loaded {loaded_files}/{len(files)} files\")\n",
    "        \n",
    "        if not all_data:\n",
    "            raise ValueError(\"No valid data loaded - check file formats\")\n",
    "            \n",
    "        avg_sfreq = np.mean(sfreqs) if sfreqs else (100 if is_patient else 250)\n",
    "        return np.concatenate(all_data), np.concatenate(all_labels), avg_sfreq\n",
    "\n",
    "    def load_brainvision_data(self, vhdr_path, max_duration=None):\n",
    "        \"\"\"Load BrainVision data with enhanced validation\"\"\"\n",
    "        try:\n",
    "            raw = mne.io.read_raw_brainvision(vhdr_path, preload=True, verbose=False)\n",
    "            \n",
    "            if max_duration:\n",
    "                crop_end = min(max_duration, raw.times[-1])\n",
    "                raw.crop(tmax=crop_end)\n",
    "                \n",
    "            # Apply standard preprocessing to match patient data\n",
    "            raw.filter(0.5, 45, fir_design='firwin', phase='zero-double')\n",
    "            raw.notch_filter(np.arange(50, 251, 50), filter_length='auto', phase='zero')\n",
    "            \n",
    "            data = raw.get_data().T.astype(np.float32)\n",
    "            events, _ = mne.events_from_annotations(raw)\n",
    "            labels = events[:, 2] if len(events) > 0 else np.zeros(len(data))\n",
    "            \n",
    "            return data, labels, raw.info['sfreq']\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading {vhdr_path}: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return np.array([]), np.array([]), None\n",
    "\n",
    "    def load_mat_data(self, mat_path, max_duration=None, default_sfreq=100):\n",
    "        \"\"\"Flexible MAT file loader supporting multiple structures\"\"\"\n",
    "        try:\n",
    "            mat_data = loadmat(mat_path)\n",
    "            eeg_data = None\n",
    "            labels = None\n",
    "            sfreq = default_sfreq\n",
    "            \n",
    "            if 'data' in mat_data:\n",
    "                data_struct = mat_data['data'][0][0]\n",
    "                if 'X' in data_struct.dtype.names:\n",
    "                    eeg_data = data_struct['X']\n",
    "                    if eeg_data.ndim > 2:\n",
    "                        eeg_data = eeg_data.reshape(eeg_data.shape[0], -1)\n",
    "                    if 'y' in data_struct.dtype.names:\n",
    "                        labels = data_struct['y'].flatten()\n",
    "                    if 'sfreq' in data_struct.dtype.names:\n",
    "                        sfreq = float(data_struct['sfreq'][0][0])\n",
    "                    elif 'Fs' in data_struct.dtype.names:\n",
    "                        sfreq = float(data_struct['Fs'][0][0])\n",
    "            \n",
    "            elif 'EEG' in mat_data:\n",
    "                eeg_struct = mat_data['EEG'][0][0]\n",
    "                if 'data' in eeg_struct.dtype.names:\n",
    "                    eeg_data = eeg_struct['data'].T\n",
    "                if 'event' in eeg_struct.dtype.names:\n",
    "                    events = eeg_struct['event'][0]\n",
    "                    labels = np.array([ev[0]['type'][0] for ev in events])\n",
    "                if 'srate' in eeg_struct.dtype.names:\n",
    "                    sfreq = float(eeg_struct['srate'][0][0])\n",
    "            \n",
    "            elif 'X' in mat_data:\n",
    "                eeg_data = mat_data['X']\n",
    "                if 'y' in mat_data:\n",
    "                    labels = mat_data['y'].flatten()\n",
    "            \n",
    "            if labels is None or len(np.unique(labels)) <= 1:\n",
    "                labels = np.zeros(len(eeg_data)) if eeg_data is not None else np.array([])\n",
    "            \n",
    "            if eeg_data is None:\n",
    "                raise ValueError(\"No EEG data found in MAT file\")\n",
    "            \n",
    "            min_len = min(len(eeg_data), len(labels))\n",
    "            eeg_data = eeg_data[:min_len]\n",
    "            labels = labels[:min_len]\n",
    "            \n",
    "            if max_duration:\n",
    "                max_samples = int(max_duration * sfreq)\n",
    "                eeg_data = eeg_data[:max_samples]\n",
    "                labels = labels[:max_samples]\n",
    "                \n",
    "            return eeg_data.astype(np.float32), labels, sfreq\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading {mat_path}: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return np.array([]), np.array([]), None\n",
    "\n",
    "    def hjorth_parameters(self, signal):\n",
    "        \"\"\"Calculate Hjorth mobility and complexity parameters\"\"\"\n",
    "        first_deriv = np.diff(signal)\n",
    "        second_deriv = np.diff(signal, 2)\n",
    "        \n",
    "        var_zero = np.var(signal)\n",
    "        var_d1 = np.var(first_deriv)\n",
    "        var_d2 = np.var(second_deriv)\n",
    "        \n",
    "        mobility = np.sqrt(var_d1 / var_zero)\n",
    "        complexity = np.sqrt(var_d2 / var_d1) / mobility\n",
    "        \n",
    "        return mobility, complexity\n",
    "\n",
    "    def preprocess_data(self, data, labels, sfreq=250, common_channels=None):\n",
    "        \"\"\"Enhanced preprocessing with detailed debugging\"\"\"\n",
    "        print(\"\\n=== Starting Preprocessing ===\")\n",
    "        print(f\"Input data shape: {data.shape if data is not None else 'None'}\")\n",
    "        print(f\"Sample rate: {sfreq} Hz\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Validate input data\n",
    "            if data is None or len(data) == 0:\n",
    "                raise ValueError(\"Empty data array received\")\n",
    "                \n",
    "            if len(data.shape) != 2:\n",
    "                raise ValueError(f\"Expected 2D array, got {len(data.shape)}D array\")\n",
    "                \n",
    "            n_channels = data.shape[1]\n",
    "            print(f\"Number of channels: {n_channels}\")\n",
    "            \n",
    "            if n_channels == 0:\n",
    "                raise ValueError(\"No channels found in data\")\n",
    "    \n",
    "            # 2. Create RawArray\n",
    "            ch_names = common_channels[:n_channels] if common_channels else [f'ch{i}' for i in range(n_channels)]\n",
    "            print(f\"First 3 channel names: {ch_names[:3]}\")\n",
    "            \n",
    "            info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')\n",
    "            print(\"Created info structure\")\n",
    "            \n",
    "            try:\n",
    "                raw = mne.io.RawArray(data.T, info)\n",
    "                print(\"Successfully created RawArray\")\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Failed to create RawArray: {str(e)}\")\n",
    "    \n",
    "            # 3. Apply filters\n",
    "            nyquist = sfreq / 2\n",
    "            print(f\"Nyquist frequency: {nyquist} Hz\")\n",
    "            \n",
    "            try:\n",
    "                print(\"Applying bandpass filter...\")\n",
    "                raw.filter(0.5, min(45, nyquist-1), fir_design='firwin', phase='zero-double')\n",
    "                \n",
    "                print(\"Applying notch filter...\")\n",
    "                notch_freqs = [f for f in np.arange(50, 251, 50) if f < nyquist]\n",
    "                if notch_freqs:\n",
    "                    raw.notch_filter(notch_freqs, filter_length='auto', phase='zero')\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Filtering failed: {str(e)}\")\n",
    "    \n",
    "            # 4. Create epochs\n",
    "            try:\n",
    "                print(\"Creating epochs...\")\n",
    "                events = mne.make_fixed_length_events(raw, duration=1.0)\n",
    "                print(f\"Created {len(events)} events\")\n",
    "                \n",
    "                epochs = mne.Epochs(raw, events, tmin=0, tmax=1.0, baseline=None, preload=True)\n",
    "                epochs_data = epochs.get_data()\n",
    "                print(f\"Epochs shape: {epochs_data.shape}\")\n",
    "                \n",
    "                if len(epochs_data) == 0:\n",
    "                    raise ValueError(\"No epochs created - check duration parameters\")\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Epoch creation failed: {str(e)}\")\n",
    "    \n",
    "            # 5. Feature extraction\n",
    "            print(\"Extracting features...\")\n",
    "            X, y = self._extract_features(epochs_data, labels[:len(epochs_data)], sfreq, nyquist)\n",
    "            print(f\"Extracted features shape: {X.shape}\")\n",
    "            \n",
    "            if len(X) == 0:\n",
    "                raise ValueError(\"Feature extraction returned empty array\")\n",
    "                \n",
    "            return X, y\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Preprocessing failed: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            self._error_messages.append(error_msg)\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "    def _extract_features(self, epochs_data, labels, sfreq, nyquist):\n",
    "            \"\"\"Internal feature extraction method\"\"\"\n",
    "            features = []\n",
    "            n_channels = epochs_data.shape[1]\n",
    "            \n",
    "            for epoch in epochs_data:\n",
    "                epoch_features = []\n",
    "                for channel in epoch:\n",
    "                    # Frequency features\n",
    "                    try:\n",
    "                        freqs, psd = welch(channel, fs=sfreq, nperseg=min(256, len(channel)))\n",
    "                        bands = {'delta': (0.5, 4), 'theta': (4, 8), 'alpha': (8, 12),\n",
    "                                'beta': (12, 30), 'gamma': (30, min(45, nyquist-1))}\n",
    "                        band_powers = [np.sum(psd[(freqs >= low) & (freqs <= high)]) \n",
    "                                     for low, high in bands.values()]\n",
    "                    except:\n",
    "                        band_powers = [0.0] * len(self.band_names)\n",
    "        \n",
    "                    # Wavelet features\n",
    "                    try:\n",
    "                        coeffs = pywt.wavedec(channel, 'db4', level=4)\n",
    "                        wavelet_features = [np.mean(c) for c in coeffs[:5]]\n",
    "                        if len(wavelet_features) < 5:\n",
    "                            wavelet_features += [0.0] * (5 - len(wavelet_features))\n",
    "                    except:\n",
    "                        wavelet_features = [0.0] * 5\n",
    "        \n",
    "                    # Statistical features\n",
    "                    stats = [\n",
    "                        np.mean(channel),\n",
    "                        np.std(channel),\n",
    "                        np.median(channel),\n",
    "                        pd.Series(channel).skew(),\n",
    "                        pd.Series(channel).kurtosis()\n",
    "                    ]\n",
    "                    \n",
    "                    # Hjorth parameters\n",
    "                    try:\n",
    "                        mobility, complexity = self.hjorth_parameters(channel)\n",
    "                        hjorth_features = [mobility, complexity]\n",
    "                    except:\n",
    "                        hjorth_features = [0.0, 0.0]\n",
    "        \n",
    "                    epoch_features.extend(band_powers + wavelet_features + stats + hjorth_features)\n",
    "                \n",
    "                features.append(epoch_features)\n",
    "            \n",
    "            X = np.array(features)\n",
    "            y = np.array(labels)\n",
    "            \n",
    "            # Handle empty case\n",
    "            if len(X) == 0:\n",
    "                return np.empty((0, self.expected_features_per_channel * n_channels)), np.array([])\n",
    "            \n",
    "            return X, y\n",
    "\n",
    "    def load_preprocessing_artifacts(self, scaler_path, label_encoder_path):\n",
    "        \"\"\"Load preprocessing artifacts with dimension validation\"\"\"\n",
    "        try:\n",
    "            self.scaler = joblib.load(scaler_path)\n",
    "            print(f\"✓ Scaler loaded successfully (expecting {self.scaler.n_features_in_} features)\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"✗ Failed to load scaler: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            self.scaler = None\n",
    "            \n",
    "        try:\n",
    "            self.label_encoder = joblib.load(label_encoder_path)\n",
    "            print(\"✓ Label encoder loaded successfully\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"✗ Failed to load label encoder: {str(e)}\"\n",
    "            self._error_messages.append(error_msg)\n",
    "            print(error_msg)\n",
    "            self.label_encoder = None\n",
    "\n",
    "    def match_features(self, X, expected_features):\n",
    "        \"\"\"Ensure feature matrix matches expected dimensions\"\"\"\n",
    "        if X is None or len(X) == 0:\n",
    "            return np.array([])\n",
    "            \n",
    "        if X.shape[1] == expected_features:\n",
    "            return X\n",
    "        elif X.shape[1] < expected_features:\n",
    "            pad_width = ((0, 0), (0, expected_features - X.shape[1]))\n",
    "            return np.pad(X, pad_width, mode='constant')\n",
    "        else:\n",
    "            return X[:, :expected_features]\n",
    "\n",
    "    def load_models(self, model_paths):\n",
    "        \"\"\"Load models with comprehensive validation\"\"\"\n",
    "        self.models = {}\n",
    "        for name, path in model_paths.items():\n",
    "            try:\n",
    "                self.models[name] = joblib.load(path)\n",
    "                print(f\"✓ {name} model loaded successfully\")\n",
    "            except Exception as e:\n",
    "                error_msg = f\"✗ Failed to load {name} model: {str(e)}\"\n",
    "                self._error_messages.append(error_msg)\n",
    "                print(error_msg)\n",
    "                \n",
    "        if not self.models:\n",
    "            raise ValueError(\"No models were loaded successfully\")\n",
    "        return self.models\n",
    "\n",
    "    def evaluate_models(self, X_data, y_data, data_type='Patient'):\n",
    "        \"\"\"Enhanced evaluation with better ROC curve handling\"\"\"\n",
    "        self.results[data_type] = {}\n",
    "        for name, model in self.models.items():\n",
    "            try:\n",
    "                y_pred = model.predict(X_data)\n",
    "                y_proba = model.predict_proba(X_data)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "                \n",
    "                acc = accuracy_score(y_data, y_pred)\n",
    "                report = classification_report(y_data, y_pred, output_dict=True, zero_division=0)\n",
    "                cm = confusion_matrix(y_data, y_pred)\n",
    "                \n",
    "                # ROC curve calculation\n",
    "                roc_auc, fpr, tpr = None, None, None\n",
    "                if y_proba is not None and len(np.unique(y_data)) == 2:\n",
    "                    fpr, tpr, _ = roc_curve(y_data, y_proba)\n",
    "                    roc_auc = auc(fpr, tpr)\n",
    "                \n",
    "                # Precision-Recall curve\n",
    "                pr_auc, precision, recall = None, None, None\n",
    "                if y_proba is not None and len(np.unique(y_data)) == 2:\n",
    "                    precision, recall, _ = precision_recall_curve(y_data, y_proba)\n",
    "                    pr_auc = average_precision_score(y_data, y_proba)\n",
    "                \n",
    "                self.results[data_type][name] = {\n",
    "                    'accuracy': acc,\n",
    "                    'report': report,\n",
    "                    'confusion_matrix': cm,\n",
    "                    'roc_auc': roc_auc,\n",
    "                    'fpr': fpr,\n",
    "                    'tpr': tpr,\n",
    "                    'pr_auc': pr_auc,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'y_true': y_data,\n",
    "                    'y_pred': y_pred,\n",
    "                    'y_proba': y_proba\n",
    "                }\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error evaluating {name} on {data_type} data: {str(e)}\"\n",
    "                self._error_messages.append(error_msg)\n",
    "                print(error_msg)\n",
    "\n",
    "    def plot_eeg_comparison(self, healthy_data, patient_data, healthy_sfreq=250, patient_sfreq=100, samples=500, channels=3):\n",
    "        \"\"\"Enhanced EEG signal comparison\"\"\"\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), sharey=True)\n",
    "        \n",
    "        time_h = np.arange(min(samples, len(healthy_data))) / healthy_sfreq\n",
    "        time_p = np.arange(min(samples, len(patient_data))) / patient_sfreq\n",
    "        \n",
    "        # Healthy EEG\n",
    "        for ch in range(min(channels, healthy_data.shape[1])):\n",
    "            ax1.plot(time_h, healthy_data[:len(time_h), ch] * 1e6,\n",
    "                    linewidth=1.5, alpha=0.8, label=f'Ch{ch+1}')\n",
    "        \n",
    "        ax1.set_title('Healthy EEG', fontsize=16, pad=20)\n",
    "        ax1.set_xlabel('Time (s)', fontsize=14)\n",
    "        ax1.set_ylabel('Amplitude (μV)', fontsize=14)\n",
    "        ax1.legend(loc='upper right', fontsize=10)\n",
    "        ax1.grid(True, linestyle=':', alpha=0.5)\n",
    "        ax1.set_ylim(-150, 150)\n",
    "        \n",
    "        # Patient EEG\n",
    "        for ch in range(min(channels, patient_data.shape[1])):\n",
    "            ax2.plot(time_p, patient_data[:len(time_p), ch] * 1e6,\n",
    "                    linewidth=1.5, alpha=0.8, label=f'Ch{ch+1}')\n",
    "        \n",
    "        ax2.set_title('Patient EEG', fontsize=16, pad=20)\n",
    "        ax2.set_xlabel('Time (s)', fontsize=14)\n",
    "        ax2.legend(loc='upper right', fontsize=10)\n",
    "        ax2.grid(True, linestyle=':', alpha=0.5)\n",
    "        ax2.set_ylim(-150, 150)\n",
    "        \n",
    "        plt.suptitle('EEG Signal Comparison', fontsize=18, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_roc_curves(self):\n",
    "        \"\"\"Enhanced ROC curve plotting with AUC values\"\"\"\n",
    "        if not self.results or 'Patient' not in self.results:\n",
    "            print(\"No patient results available for ROC curves\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for model_name, results in self.results['Patient'].items():\n",
    "            if results.get('roc_auc') is not None:\n",
    "                plt.plot(results['fpr'], results['tpr'],\n",
    "                        lw=2, label=f'{model_name} (AUC = {results[\"roc_auc\"]:.2f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate', fontsize=12)\n",
    "        plt.ylabel('True Positive Rate', fontsize=12)\n",
    "        plt.title('Receiver Operating Characteristic', fontsize=16)\n",
    "        plt.legend(loc=\"lower right\", fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_precision_recall_curves(self):\n",
    "        \"\"\"Plot precision-recall curves\"\"\"\n",
    "        if not self.results or 'Patient' not in self.results:\n",
    "            print(\"No patient results available for PR curves\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for model_name, results in self.results['Patient'].items():\n",
    "            if results.get('pr_auc') is not None:\n",
    "                plt.plot(results['recall'], results['precision'],\n",
    "                        lw=2, label=f'{model_name} (AP = {results[\"pr_auc\"]:.2f})')\n",
    "        \n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Recall', fontsize=12)\n",
    "        plt.ylabel('Precision', fontsize=12)\n",
    "        plt.title('Precision-Recall Curve', fontsize=16)\n",
    "        plt.legend(loc=\"upper right\", fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_feature_importance(self):\n",
    "        \"\"\"Plot feature importance if available\"\"\"\n",
    "        for model_name, model in self.models.items():\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importances = model.feature_importances_\n",
    "                indices = np.argsort(importances)[::-1]\n",
    "                \n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.title(f\"Feature Importances - {model_name}\", fontsize=16)\n",
    "                plt.bar(range(20), importances[indices[:20]], align=\"center\")\n",
    "                plt.xticks(range(20), indices[:20], rotation=45)\n",
    "                plt.xlim([-1, 20])\n",
    "                plt.ylabel(\"Importance\", fontsize=12)\n",
    "                plt.xlabel(\"Feature Index\", fontsize=12)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "    def run_pipeline(self, config):\n",
    "        \"\"\"Enhanced pipeline with better error handling\"\"\"\n",
    "        results = None\n",
    "        try:\n",
    "            print(\"\\n=== EEG Analysis Pipeline ===\\n\")\n",
    "            print(\"\\n=== Data Validation ===\")\n",
    "            print(\"Healthy files:\")\n",
    "            healthy_files = [f for f in os.listdir(config['healthy_path']) if f.endswith('.vhdr')]\n",
    "            print(f\"Found {len(healthy_files)} files\")\n",
    "            print(\"Sample healthy file:\", healthy_files[0] if healthy_files else \"None\")\n",
    "            \n",
    "            print(\"\\nPatient files:\")\n",
    "            patient_files = [f for f in os.listdir(config['patient_path']) if f.endswith('.mat')]\n",
    "            print(f\"Found {len(patient_files)} files\")\n",
    "            print(\"Sample patient file:\", patient_files[0] if patient_files else \"None\")\n",
    "            # 1. Load data\n",
    "            print(\"[1/7] Loading data...\")\n",
    "            healthy_data, healthy_labels, h_sfreq = self.load_dataset(\n",
    "                config['healthy_path'],\n",
    "                max_files=config.get('max_files'),\n",
    "                max_duration=config.get('max_duration', None))\n",
    "            patient_data, patient_labels, p_sfreq = self.load_dataset(\n",
    "                config['patient_path'],\n",
    "                is_patient=True,\n",
    "                max_files=config.get('max_files'),\n",
    "                max_duration=config.get('max_duration', None))\n",
    "            \n",
    "            # 2. Channel matching\n",
    "            print(\"[2/7] Finding common channels...\")\n",
    "            if not config.get('skip_channel_matching', False):\n",
    "                healthy_channels = self.collect_channel_names(config['healthy_path'])\n",
    "                patient_channels = self.collect_channel_names(config['patient_path'], is_patient=True)\n",
    "                self.common_channels = self.find_common_channels(healthy_channels, patient_channels)\n",
    "            \n",
    "            # 3. Preprocessing\n",
    "            print(\"[3/7] Preprocessing data...\")\n",
    "            X_healthy, y_healthy = self.preprocess_data(\n",
    "                healthy_data, healthy_labels,\n",
    "                sfreq=h_sfreq,\n",
    "                common_channels=self.common_channels)\n",
    "            X_patients, y_patients = self.preprocess_data(\n",
    "                patient_data, patient_labels,\n",
    "                sfreq=p_sfreq,\n",
    "                common_channels=self.common_channels)\n",
    "            \n",
    "            # 4. Load artifacts\n",
    "            print(\"[4/7] Loading preprocessing artifacts...\")\n",
    "            self.load_preprocessing_artifacts(\n",
    "                config['scaler_path'],\n",
    "                config['label_encoder_path'])\n",
    "            \n",
    "            # 5. Apply transformations\n",
    "            # 5. Apply transformations\n",
    "            print(\"[5/7] Applying transformations...\")\n",
    "            if self.scaler:\n",
    "                # Validate data before transformation\n",
    "                if len(X_healthy) == 0 or len(X_patients) == 0:\n",
    "                    raise ValueError(\"Empty feature arrays - check preprocessing output\")\n",
    "                \n",
    "                try:\n",
    "                    X_healthy = self.scaler.transform(X_healthy)\n",
    "                    X_patients = self.scaler.transform(X_patients)\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Scaling failed: {str(e)}\"\n",
    "                    self._error_messages.append(error_msg)\n",
    "                    raise ValueError(error_msg)\n",
    "            \n",
    "            if self.label_encoder:\n",
    "                try:\n",
    "                    y_patients = self.label_encoder.transform(y_patients)\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Label encoding failed: {str(e)}\"\n",
    "                    self._error_messages.append(error_msg)\n",
    "                    raise ValueError(error_msg)\n",
    "            \n",
    "            # 6. Load models\n",
    "            print(\"[6/7] Loading models...\")\n",
    "            self.load_models(config['model_paths'])\n",
    "            \n",
    "            # 7. Evaluate models\n",
    "            print(\"[7/7] Evaluating models...\")\n",
    "            self.evaluate_models(X_patients, y_patients, data_type='Patient')\n",
    "            \n",
    "            # Generate visualizations\n",
    "            print(\"\\n=== Generating Visualizations ===\\n\")\n",
    "            try:\n",
    "                plt.style.use('seaborn-v0_8-whitegrid')\n",
    "            except:\n",
    "                plt.style.use('ggplot')\n",
    "            \n",
    "            vis_funcs = [\n",
    "                ('EEG Comparison', self.plot_eeg_comparison, [healthy_data, patient_data, h_sfreq, p_sfreq]),\n",
    "                ('Patient Response', self.plot_patient_response_categories, []),\n",
    "                ('Confusion Matrix', self.plot_confusion_matrices, ['Patient']),\n",
    "                ('Model Performance', self.plot_model_performance_comparison, []),\n",
    "                ('ROC Curves', self.plot_roc_curves, []),\n",
    "                ('PR Curves', self.plot_precision_recall_curves, []),\n",
    "                ('Feature Importance', self.plot_feature_importance, [])\n",
    "            ]\n",
    "            \n",
    "            for name, func, args in vis_funcs:\n",
    "                try:\n",
    "                    print(f\"Generating {name} visualization...\")\n",
    "                    func(*args)\n",
    "                    plt.close('all')\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Failed to generate {name}: {str(e)}\"\n",
    "                    self._error_messages.append(error_msg)\n",
    "                    print(error_msg)\n",
    "            \n",
    "            print(\"\\n=== Pipeline Completed ===\\n\")\n",
    "            if self._error_messages:\n",
    "                print(\"Completed with warnings/errors (see above messages)\")\n",
    "            else:\n",
    "                print(\"Completed successfully!\")\n",
    "            \n",
    "            results = {\n",
    "                'healthy_data': (X_healthy, y_healthy),\n",
    "                'patient_data': (X_patients, y_patients),\n",
    "                'results': self.results,\n",
    "                'metadata': {\n",
    "                    'healthy_samples': len(X_healthy),\n",
    "                    'patient_samples': len(X_patients),\n",
    "                    'common_channels': self.common_channels,\n",
    "                    'features_per_channel': self.expected_features_per_channel,\n",
    "                    'errors': self._error_messages\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n!!! Pipeline Failed !!!\\nError: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "        finally:\n",
    "            if 'healthy_data' in locals():\n",
    "                del healthy_data\n",
    "            if 'patient_data' in locals():\n",
    "                del patient_data\n",
    "            gc.collect()\n",
    "            \n",
    "        return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    channel_mapping = {\n",
    "        'FP1': 'Fp1',\n",
    "        'FP2': 'Fp2',\n",
    "        'F3': 'F3',\n",
    "        'F4': 'F4',\n",
    "    }\n",
    "\n",
    "    config = {\n",
    "        'healthy_path': \"F:/shivani/VSCode/ml/worked on dataset/3(final)/Healthy\",\n",
    "        'patient_path': \"F:/shivani/VSCode/ml/worked on dataset/4/dataset/Patients\",\n",
    "        'model_paths': {\n",
    "            \"SVM\": \"F:/shivani/VSCode/ml/worked on dataset/3(final)/svm_model.pkl\",\n",
    "            \"RF\": \"F:/shivani/VSCode/ml/worked on dataset/3(final)/rf_model.pkl\",\n",
    "            \"XGB\": \"F:/shivani/VSCode/ml/worked on dataset/3(final)/xgb_model.pkl\"\n",
    "        },\n",
    "        'scaler_path': \"F:/shivani/VSCode/ml/worked on dataset/3(final)/preprocessing_artifacts/eeg_scaler.joblib\",\n",
    "        'label_encoder_path': \"F:/shivani/VSCode/ml/worked on dataset/3(final)/preprocessing_artifacts/label_encoder.joblib\",\n",
    "        'max_files': 5,\n",
    "    }\n",
    "\n",
    "    pipeline = EEGPipeline()\n",
    "    pipeline.set_channel_mapping(channel_mapping)\n",
    "    results = pipeline.run_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import mne\n",
    "# import gc\n",
    "# from scipy.io import loadmat\n",
    "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# from sklearn.metrics import (\n",
    "#     accuracy_score, classification_report, confusion_matrix,\n",
    "#     roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "# )\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import joblib\n",
    "# from scipy.signal import welch\n",
    "# import pywt\n",
    "# import pandas as pd\n",
    "# from matplotlib.gridspec import GridSpec\n",
    "# import re\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from matplotlib.ticker import FormatStrFormatter\n",
    "# from keras.models import load_model\n",
    "\n",
    "# HAS_ENTROPY_PACKAGE = False\n",
    "# try:\n",
    "#     from entropy import sample_entropy as entropy_sample_entropy, spectral_entropy as entropy_spectral_entropy\n",
    "#     HAS_ENTROPY_PACKAGE = True\n",
    "# except ImportError:\n",
    "#     print(\"Entropy package not found - using custom implementations\")\n",
    "#     HAS_ENTROPY_PACKAGE = False\n",
    "    \n",
    "# # Configure plotting\n",
    "# plt.rcParams['figure.figsize'] = [12, 8]\n",
    "# plt.rcParams['font.size'] = 12\n",
    "# sns.set_style(\"whitegrid\")\n",
    "# sns.set_palette(\"colorblind\")\n",
    "\n",
    "# class EEGPipeline:\n",
    "#     def __init__(self):\n",
    "#         self.models = {}\n",
    "#         self.results = {}\n",
    "#         self.scaler = None\n",
    "#         self.label_encoder = None\n",
    "#         self.common_channels = None\n",
    "#         self.feature_names = [\n",
    "#             'Delta Power', 'Theta Power', 'Alpha Power', 'Beta Power', 'Gamma Power',\n",
    "#             'Wavelet Mean 1', 'Wavelet Mean 2', 'Wavelet Mean 3', 'Wavelet Mean 4', 'Wavelet Mean 5',\n",
    "#             'Mean', 'Std Dev', 'Median', 'Sample Entropy', 'Spectral Entropy'\n",
    "#         ]\n",
    "#         self.band_names = ['Delta', 'Theta', 'Alpha', 'Beta', 'Gamma']\n",
    "#         self.channel_mapping = {}\n",
    "#         self.expected_features_per_channel = 15  # Updated to include entropy features\n",
    "#         self._error_messages = []  # Track errors during pipeline execution\n",
    "\n",
    "#     def custom_sample_entropy(self, signal, m=2, r_factor=0.2):\n",
    "#         \"\"\"Custom implementation of sample entropy\"\"\"\n",
    "#         n = len(signal)\n",
    "#         r = r_factor * np.std(signal)\n",
    "        \n",
    "#         def _maxdist(x, y):\n",
    "#             return np.max(np.abs(x - y))\n",
    "            \n",
    "#         def _phi(m):\n",
    "#             x = np.array([signal[i:i+m] for i in range(n - m + 1)])\n",
    "#             C = np.zeros(len(x))\n",
    "#             for i in range(len(x)):\n",
    "#                 for j in range(len(x)):\n",
    "#                     if i != j and _maxdist(x[i], x[j]) <= r:\n",
    "#                         C[i] += 1\n",
    "#             return np.sum(C) / (len(x) * (len(x) - 1))\n",
    "            \n",
    "#         if n == 0 or m > n:\n",
    "#             return 0.0\n",
    "            \n",
    "#         return -np.log(_phi(m+1) / _phi(m)) if _phi(m) != 0 else 0.0\n",
    "\n",
    "#     def custom_spectral_entropy(self, signal, sfreq, bands=None):\n",
    "#         \"\"\"Custom implementation of spectral entropy\"\"\"\n",
    "#         if bands is None:\n",
    "#             bands = {\n",
    "#                 'delta': (0.5, 4),\n",
    "#                 'theta': (4, 8),\n",
    "#                 'alpha': (8, 12),\n",
    "#                 'beta': (12, 30),\n",
    "#                 'gamma': (30, 50)\n",
    "#             }\n",
    "        \n",
    "#         freqs, psd = welch(signal, fs=sfreq, nperseg=min(256, len(signal)))\n",
    "#         total_power = np.sum(psd)\n",
    "        \n",
    "#         if total_power == 0:\n",
    "#             return 0.0\n",
    "            \n",
    "#         prob = psd / total_power\n",
    "#         prob = prob[prob > 0]  # Avoid log(0)\n",
    "        \n",
    "#         # Calculate entropy for each band\n",
    "#         band_entropies = []\n",
    "#         for low, high in bands.values():\n",
    "#             band_mask = (freqs >= low) & (freqs <= high)\n",
    "#             band_prob = prob[band_mask]\n",
    "#             if len(band_prob) > 0:\n",
    "#                 band_entropy = -np.sum(band_prob * np.log(band_prob))\n",
    "#                 band_entropies.append(band_entropy)\n",
    "        \n",
    "#         return np.sum(band_entropies) if band_entropies else 0.0\n",
    "\n",
    "#     def calculate_sample_entropy(self, signal, m=2, r_factor=0.2):\n",
    "#         \"\"\"Calculate sample entropy using available implementation\"\"\"\n",
    "#         if HAS_ENTROPY_PACKAGE:\n",
    "#             return entropy_sample_entropy(signal, m=m, r=r_factor*np.std(signal))\n",
    "#         return self.custom_sample_entropy(signal, m=m, r_factor=r_factor)\n",
    "\n",
    "#     def calculate_spectral_entropy(self, signal, sfreq):\n",
    "#         \"\"\"Calculate spectral entropy using available implementation\"\"\"\n",
    "#         bands = {'delta': (0.5, 4), 'theta': (4, 8), 'alpha': (8, 12),\n",
    "#                 'beta': (12, 30), 'gamma': (30, 50)}\n",
    "#         if HAS_ENTROPY_PACKAGE:\n",
    "#             return entropy_spectral_entropy(signal, sfreq, bands)\n",
    "#         return self.custom_spectral_entropy(signal, sfreq, bands)\n",
    "\n",
    "#     def set_channel_mapping(self, mapping_dict):\n",
    "#         \"\"\"Set manual channel name mapping between different naming conventions\"\"\"\n",
    "#         self.channel_mapping = mapping_dict\n",
    "\n",
    "#     def normalize_channel_name(self, channel_name):\n",
    "#         \"\"\"Advanced channel name normalization with manual mapping support\"\"\"\n",
    "#         if isinstance(channel_name, (list, np.ndarray)):\n",
    "#             channel_name = channel_name[0]\n",
    "#         channel_name = str(channel_name).strip().upper()\n",
    "#         if channel_name in self.channel_mapping:\n",
    "#             return self.channel_mapping[channel_name]\n",
    "#         channel_name = re.sub(r'[^A-Z0-9]', '', channel_name)\n",
    "#         channel_name = re.sub(r'^CH', '', channel_name)\n",
    "#         channel_name = re.sub(r'^EEG', '', channel_name)\n",
    "#         channel_name = channel_name.lstrip('0')\n",
    "#         # Handle common variations\n",
    "#         variations = {\n",
    "#             'FP1': 'Fp1', 'FP2': 'Fp2',\n",
    "#             'T3': 'T7', 'T4': 'T8',\n",
    "#             'T5': 'P7', 'T6': 'P8'\n",
    "#         }\n",
    "#         return variations.get(channel_name, channel_name)\n",
    "\n",
    "#     def get_channel_names_from_mat(self, mat_path):\n",
    "#         \"\"\"Robust MAT file channel extraction supporting multiple formats\"\"\"\n",
    "#         try:\n",
    "#             mat_data = loadmat(mat_path)\n",
    "#             channels = []\n",
    "#             # Structure 1: Nested 'data' structure\n",
    "#             if 'data' in mat_data:\n",
    "#                 data_struct = mat_data['data'][0][0]\n",
    "#                 if 'channels' in data_struct.dtype.names:\n",
    "#                     channels = [str(ch[0]) for ch in data_struct['channels'][0]]\n",
    "#                 elif 'chanlocs' in data_struct.dtype.names:\n",
    "#                     chanlocs = data_struct['chanlocs'][0]\n",
    "#                     channels = [str(chan['labels'][0]) for chan in chanlocs]\n",
    "#             # Structure 2: EEGLAB structure\n",
    "#             elif 'EEG' in mat_data:\n",
    "#                 eeg_struct = mat_data['EEG'][0][0]\n",
    "#                 if 'chanlocs' in eeg_struct.dtype.names:\n",
    "#                     chanlocs = eeg_struct['chanlocs'][0]\n",
    "#                     channels = [str(chan['labels'][0]) for chan in chanlocs]\n",
    "#                 elif 'chaninfo' in eeg_struct.dtype.names:\n",
    "#                     chaninfo = eeg_struct['chaninfo'][0][0]\n",
    "#                     if 'labels' in chaninfo.dtype.names:\n",
    "#                         channels = [str(ch[0]) for ch in chaninfo['labels'][0]]\n",
    "#             # Structure 3: Simple X,y structure\n",
    "#             elif 'X' in mat_data and 'ch_names' in mat_data:\n",
    "#                 channels = [str(ch[0]) for ch in mat_data['ch_names'][0]]\n",
    "#             return [self.normalize_channel_name(ch) for ch in channels if ch and str(ch).strip()]\n",
    "#         except Exception as e:\n",
    "#             error_msg = f\"Error loading {mat_path}: {str(e)}\"\n",
    "#             self._error_messages.append(error_msg)\n",
    "#             print(error_msg)\n",
    "#             return []\n",
    "\n",
    "#     def get_channel_names_from_vhdr(self, vhdr_path):\n",
    "#         \"\"\"Extract channel names from BrainVision files with validation\"\"\"\n",
    "#         try:\n",
    "#             raw = mne.io.read_raw_brainvision(vhdr_path, preload=False, verbose=False)\n",
    "#             return [self.normalize_channel_name(ch) for ch in raw.ch_names]\n",
    "#         except Exception as e:\n",
    "#             error_msg = f\"Error loading {vhdr_path}: {str(e)}\"\n",
    "#             self._error_messages.append(error_msg)\n",
    "#             print(error_msg)\n",
    "#             return []\n",
    "\n",
    "#     def find_common_channels(self, healthy_channels, patient_channels):\n",
    "#         \"\"\"Flexible channel matching with multiple strategies\"\"\"\n",
    "#         # First try exact matching\n",
    "#         common = set(healthy_channels).intersection(patient_channels)\n",
    "#         if not common:\n",
    "#             healthy_set = set(healthy_channels)\n",
    "#             patient_set = set(patient_channels)\n",
    "#             common = healthy_set.intersection(patient_set)\n",
    "#         if not common:\n",
    "#             common_partial = set()\n",
    "#             for h_ch in healthy_set:\n",
    "#                 for p_ch in patient_set:\n",
    "#                     if h_ch in p_ch or p_ch in h_ch:\n",
    "#                         common_partial.add(h_ch)\n",
    "#             if common_partial:\n",
    "#                 print(f\"Using partial channel matches: {common_partial}\")\n",
    "#                 return sorted(common_partial)\n",
    "#         print(f\"Found {len(common)} common channels\")\n",
    "#         return sorted(common)\n",
    "\n",
    "#     def collect_channel_names(self, folder_path, is_patient=False):\n",
    "#         \"\"\"Collect channel names with extensive validation\"\"\"\n",
    "#         all_channels = set()\n",
    "#         files = [f for f in os.listdir(folder_path) if f.endswith('.vhdr' if not is_patient else '.mat')]\n",
    "#         if not files:\n",
    "#             raise FileNotFoundError(f\"No valid files found in {folder_path}\")\n",
    "#         for file in tqdm(files, desc=f\"Collecting {'patient' if is_patient else 'healthy'} channels\"):\n",
    "#             file_path = os.path.join(folder_path, file)\n",
    "#             channels = self.get_channel_names_from_mat(file_path) if is_patient else self.get_channel_names_from_vhdr(file_path)\n",
    "#             if not channels:\n",
    "#                 print(f\"Warning: No channels found in {file}\")\n",
    "#                 continue\n",
    "#             all_channels.update(channels)\n",
    "#         if not all_channels:\n",
    "#             raise ValueError(f\"No channels collected from {folder_path}\")\n",
    "#         return sorted(all_channels)\n",
    "\n",
    "#     def load_dataset(self, folder_path, is_patient=False, max_files=None, max_duration=None):\n",
    "#         \"\"\"Robust dataset loading with comprehensive validation\"\"\"\n",
    "#         files = [f for f in os.listdir(folder_path) if f.endswith('.mat' if is_patient else '.vhdr')]\n",
    "#         loader_func = self.load_mat_data if is_patient else self.load_brainvision_data\n",
    "#         if not files:\n",
    "#             raise FileNotFoundError(f\"No valid files found in {folder_path}\")\n",
    "#         if max_files:\n",
    "#             files = files[:max_files]\n",
    "#         all_data = []\n",
    "#         all_labels = []\n",
    "#         sfreqs = []\n",
    "#         loaded_files = 0\n",
    "#         for file in tqdm(files, desc=f\"Loading {'patient' if is_patient else 'healthy'} files\"):\n",
    "#             file_path = os.path.join(folder_path, file)\n",
    "#             try:\n",
    "#                 data, labels, sfreq = loader_func(file_path, max_duration)\n",
    "#                 if len(data) > 0 and len(labels) > 0:\n",
    "#                     all_data.append(data)\n",
    "#                     all_labels.append(labels)\n",
    "#                     sfreqs.append(sfreq)\n",
    "#                     loaded_files += 1\n",
    "#                 else:\n",
    "#                     print(f\"Skipping {file} - no valid data\")\n",
    "#             except Exception as e:\n",
    "#                 error_msg = f\"Error loading {file}: {str(e)}\"\n",
    "#                 self._error_messages.append(error_msg)\n",
    "#                 print(error_msg)\n",
    "#             gc.collect()\n",
    "#         print(f\"\\nSuccessfully loaded {loaded_files}/{len(files)} files\")\n",
    "#         if not all_data:\n",
    "#             raise ValueError(\"No valid data loaded - check file formats\")\n",
    "#         avg_sfreq = np.mean(sfreqs) if sfreqs else (100 if is_patient else 250)\n",
    "#         return np.concatenate(all_data), np.concatenate(all_labels), avg_sfreq\n",
    "\n",
    "#     def load_brainvision_data(self, vhdr_path, max_duration=None):\n",
    "#         \"\"\"Load BrainVision data with enhanced validation\"\"\"\n",
    "#         try:\n",
    "#             raw = mne.io.read_raw_brainvision(vhdr_path, preload=True, verbose=False)\n",
    "#             if max_duration:\n",
    "#                 crop_end = min(max_duration, raw.times[-1])\n",
    "#                 raw.crop(tmax=crop_end)\n",
    "#             data = raw.get_data().T.astype(np.float32)\n",
    "#             events, _ = mne.events_from_annotations(raw)\n",
    "#             labels = events[:, 2] if len(events) > 0 else np.zeros(len(data))\n",
    "#             return data, labels, raw.info['sfreq']\n",
    "#         except Exception as e:\n",
    "#             error_msg = f\"Error loading {vhdr_path}: {str(e)}\"\n",
    "#             self._error_messages.append(error_msg)\n",
    "#             print(error_msg)\n",
    "#             return np.array([]), np.array([]), None\n",
    "\n",
    "#     def load_mat_data(self, mat_path, max_duration=None, default_sfreq=100):\n",
    "#         \"\"\"Flexible MAT file loader supporting multiple structures\"\"\"\n",
    "#         try:\n",
    "#             mat_data = loadmat(mat_path)\n",
    "#             eeg_data = None\n",
    "#             labels = None\n",
    "#             sfreq = default_sfreq\n",
    "#             if 'data' in mat_data:\n",
    "#                 data_struct = mat_data['data'][0][0]\n",
    "#                 if 'X' in data_struct.dtype.names:\n",
    "#                     eeg_data = data_struct['X']\n",
    "#                     if eeg_data.ndim > 2:\n",
    "#                         eeg_data = eeg_data.reshape(eeg_data.shape[0], -1)\n",
    "#                     if 'y' in data_struct.dtype.names:\n",
    "#                         labels = data_struct['y'].flatten()\n",
    "#                     if 'sfreq' in data_struct.dtype.names:\n",
    "#                         sfreq = float(data_struct['sfreq'][0][0])\n",
    "#                     elif 'Fs' in data_struct.dtype.names:\n",
    "#                         sfreq = float(data_struct['Fs'][0][0])\n",
    "#             elif 'EEG' in mat_data:\n",
    "#                 eeg_struct = mat_data['EEG'][0][0]\n",
    "#                 if 'data' in eeg_struct.dtype.names:\n",
    "#                     eeg_data = eeg_struct['data'].T\n",
    "#                 if 'event' in eeg_struct.dtype.names:\n",
    "#                     events = eeg_struct['event'][0]\n",
    "#                     labels = np.array([ev[0]['type'][0] for ev in events])\n",
    "#                 if 'srate' in eeg_struct.dtype.names:\n",
    "#                     sfreq = float(eeg_struct['srate'][0][0])\n",
    "#             elif 'X' in mat_data:\n",
    "#                 eeg_data = mat_data['X']\n",
    "#                 if 'y' in mat_data:\n",
    "#                     labels = mat_data['y'].flatten()\n",
    "#             if labels is None or len(np.unique(labels)) <= 1:\n",
    "#                 labels = np.zeros(len(eeg_data)) if eeg_data is not None else np.array([])\n",
    "#             if eeg_data is None:\n",
    "#                 raise ValueError(\"No EEG data found in MAT file\")\n",
    "#             min_len = min(len(eeg_data), len(labels))\n",
    "#             eeg_data = eeg_data[:min_len]\n",
    "#             labels = labels[:min_len]\n",
    "#             if max_duration:\n",
    "#                 max_samples = int(max_duration * sfreq)\n",
    "#                 eeg_data = eeg_data[:max_samples]\n",
    "#                 labels = labels[:max_samples]\n",
    "#             return eeg_data.astype(np.float32), labels, sfreq\n",
    "#         except Exception as e:\n",
    "#             error_msg = f\"Error loading {mat_path}: {str(e)}\"\n",
    "#             self._error_messages.append(error_msg)\n",
    "#             print(error_msg)\n",
    "#             return np.array([]), np.array([]), None\n",
    "\n",
    "#     def preprocess_data(self, data, labels, sfreq=250, common_channels=None):\n",
    "#         \"\"\"Robust preprocessing with fixed feature dimensions including entropy features\"\"\"\n",
    "#         try:\n",
    "#             n_channels = data.shape[1]\n",
    "#             ch_names = common_channels[:n_channels] if common_channels and len(common_channels) >= n_channels else [f'ch{i}' for i in range(n_channels)]\n",
    "#             info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')\n",
    "#             raw = mne.io.RawArray(data.T, info)\n",
    "            \n",
    "#             # Apply bandpass filter (0.5 - 40 Hz)\n",
    "#             raw.filter(0.5, 40, fir_design='firwin', phase='zero-double')\n",
    "            \n",
    "#             # Apply notch filter only for frequencies below Nyquist (sfreq/2)\n",
    "#             notch_freqs = [f for f in [50, 60] if f < (sfreq / 2)]\n",
    "#             if notch_freqs:  # Only apply if there are valid frequencies\n",
    "#                 raw.notch_filter(notch_freqs)\n",
    "            \n",
    "#             # Create epochs\n",
    "#             events = mne.make_fixed_length_events(raw, duration=1.0)\n",
    "#             epochs = mne.Epochs(raw, events, tmin=0, tmax=1.0, baseline=None, preload=True)\n",
    "#             epochs_data = epochs.get_data()\n",
    "\n",
    "#             def extract_features(epoch_data):\n",
    "#                 features = []\n",
    "#                 for epoch in epoch_data:\n",
    "#                     epoch_features = []\n",
    "#                     for channel in epoch:\n",
    "#                         # Traditional features\n",
    "#                         freqs, psd = welch(channel, fs=sfreq, nperseg=min(256, len(channel)))\n",
    "#                         bands = {'delta': (0.5, 4), 'theta': (4, 8), 'alpha': (8, 12),\n",
    "#                                 'beta': (12, 30), 'gamma': (30, 50)}\n",
    "#                         band_powers = [np.sum(psd[(freqs >= low) & (freqs <= high)]) \n",
    "#                                      for low, high in bands.values()]\n",
    "                        \n",
    "#                         # Wavelet features with adjusted level to avoid boundary effects\n",
    "#                         max_level = pywt.dwt_max_level(len(channel), 'db4')\n",
    "#                         level = min(4, max_level) if max_level is not None else 4\n",
    "#                         coeffs = pywt.wavedec(channel, 'db4', level=level)\n",
    "#                         wavelet_features = [np.mean(c) for c in coeffs[:5]]\n",
    "#                         if len(wavelet_features) < 5:\n",
    "#                             wavelet_features += [0.0] * (5 - len(wavelet_features))\n",
    "                        \n",
    "#                         # Statistical features\n",
    "#                         stats = [np.mean(channel), np.std(channel), np.median(channel)]\n",
    "                        \n",
    "#                         # Entropy features\n",
    "#                         samp_entropy = self.calculate_sample_entropy(channel)\n",
    "#                         spec_entropy = self.calculate_spectral_entropy(channel, sfreq)\n",
    "                        \n",
    "#                         # Combine all features\n",
    "#                         epoch_features.extend(band_powers + wavelet_features + stats + [samp_entropy, spec_entropy])\n",
    "#                     features.append(epoch_features)\n",
    "#                 return np.array(features)\n",
    "\n",
    "#             X = extract_features(epochs_data)\n",
    "#             y = labels[:len(X)]\n",
    "#             expected_features = n_channels * self.expected_features_per_channel\n",
    "#             if X.shape[1] != expected_features:\n",
    "#                 if X.shape[1] < expected_features:\n",
    "#                     pad_width = ((0, 0), (0, expected_features - X.shape[1]))\n",
    "#                     X = np.pad(X, pad_width, mode='constant')\n",
    "#                 else:\n",
    "#                     X = X[:, :expected_features]\n",
    "#             return X, y\n",
    "#         except Exception as e:\n",
    "#             error_msg = f\"Error during preprocessing: {e}\"\n",
    "#             self._error_messages.append(error_msg)\n",
    "#             print(error_msg)\n",
    "#             return None, None\n",
    "\n",
    "#     def load_preprocessing_artifacts(self, scaler_path, label_encoder_path):\n",
    "#         \"\"\"Load preprocessing artifacts with dimension validation\"\"\"\n",
    "#         try:\n",
    "#             self.scaler = joblib.load(scaler_path)\n",
    "#             print(f\"✓ Scaler loaded successfully (expecting {self.scaler.n_features_in_} features)\")\n",
    "#         except Exception as e:\n",
    "#             error_msg = f\"✗ Failed to load scaler: {str(e)}\"\n",
    "#             self._error_messages.append(error_msg)\n",
    "#             print(error_msg)\n",
    "#             self.scaler = None\n",
    "#         try:\n",
    "#             self.label_encoder = joblib.load(label_encoder_path)\n",
    "#             print(\"✓ Label encoder loaded successfully\")\n",
    "#         except Exception as e:\n",
    "#             error_msg = f\"✗ Failed to load label encoder: {str(e)}\"\n",
    "#             self._error_messages.append(error_msg)\n",
    "#             print(error_msg)\n",
    "#             self.label_encoder = None\n",
    "\n",
    "#     def match_features(self, X, expected_features):\n",
    "#         \"\"\"Ensure feature matrix matches expected dimensions\"\"\"\n",
    "#         if X.shape[1] == expected_features:\n",
    "#             return X\n",
    "#         elif X.shape[1] < expected_features:\n",
    "#             pad_width = ((0, 0), (0, expected_features - X.shape[1]))\n",
    "#             return np.pad(X, pad_width, mode='constant')\n",
    "#         else:\n",
    "#             return X[:, :expected_features]\n",
    "\n",
    "#     def load_models(self, model_paths):\n",
    "#         \"\"\"Load models with comprehensive validation including Keras models\"\"\"\n",
    "#         self.models = {}\n",
    "#         for name, path in model_paths.items():\n",
    "#             try:\n",
    "#                 if path.endswith('.h5') or path.endswith('.keras'):\n",
    "#                     self.models[name] = load_model(path)\n",
    "#                     print(f\"✓ {name} (Keras) model loaded successfully\")\n",
    "#                 else:\n",
    "#                     self.models[name] = joblib.load(path)\n",
    "#                     print(f\"✓ {name} model loaded successfully\")\n",
    "#             except Exception as e:\n",
    "#                 error_msg = f\"✗ Failed to load {name} model: {str(e)}\"\n",
    "#                 self._error_messages.append(error_msg)\n",
    "#                 print(error_msg)\n",
    "#         if not self.models:\n",
    "#             raise ValueError(\"No models were loaded successfully\")\n",
    "#         return self.models\n",
    "\n",
    "#     def evaluate_models(self, X_data, y_data, data_type='Patient'):\n",
    "#         \"\"\"Updated evaluation method with multiclass support and Keras model handling\"\"\"\n",
    "#         self.results[data_type] = {}\n",
    "#         for name, model in self.models.items():\n",
    "#             try:\n",
    "#                 # Handle Keras models differently\n",
    "#                 if hasattr(model, 'predict'):\n",
    "#                     # For Keras models, we need to reshape the data if needed\n",
    "#                     if len(X_data.shape) == 2:  # If it's 2D, reshape for LSTM/CNN\n",
    "#                         if 'LSTM' in name or 'CNN' in name:\n",
    "#                             X_reshaped = X_data.reshape(X_data.shape[0], X_data.shape[1], 1)\n",
    "#                             y_pred = model.predict(X_reshaped)\n",
    "#                             if y_pred.shape[1] > 1:  # Multiclass\n",
    "#                                 y_pred = np.argmax(y_pred, axis=1)\n",
    "#                             else:  # Binary\n",
    "#                                 y_pred = (y_pred > 0.5).astype(int)\n",
    "#                         else:\n",
    "#                             y_pred = model.predict(X_data)\n",
    "#                             if y_pred.shape[1] > 1:\n",
    "#                                 y_pred = np.argmax(y_pred, axis=1)\n",
    "#                             else:\n",
    "#                                 y_pred = (y_pred > 0.5).astype(int)\n",
    "#                     else:\n",
    "#                         y_pred = model.predict(X_data)\n",
    "#                         if y_pred.shape[1] > 1:\n",
    "#                             y_pred = np.argmax(y_pred, axis=1)\n",
    "#                         else:\n",
    "#                             y_pred = (y_pred > 0.5).astype(int)\n",
    "#                 else:\n",
    "#                     y_pred = model.predict(X_data)\n",
    "                \n",
    "#                 if len(np.unique(y_data)) > 2:\n",
    "#                     acc = accuracy_score(y_data, y_pred)\n",
    "#                     report = classification_report(y_data, y_pred, output_dict=True, zero_division=0)\n",
    "#                     cm = confusion_matrix(y_data, y_pred)\n",
    "#                     roc_auc = None\n",
    "#                 else:\n",
    "#                     if hasattr(model, 'predict_proba'):\n",
    "#                         y_proba = model.predict_proba(X_data)[:, 1]\n",
    "#                     elif hasattr(model, 'predict'):\n",
    "#                         if len(X_data.shape) == 2 and ('LSTM' in name or 'CNN' in name):\n",
    "#                             X_reshaped = X_data.reshape(X_data.shape[0], X_data.shape[1], 1)\n",
    "#                             y_proba = model.predict(X_reshaped).flatten()\n",
    "#                         else:\n",
    "#                             y_proba = model.predict(X_data).flatten()\n",
    "#                     else:\n",
    "#                         y_proba = None\n",
    "                    \n",
    "#                     acc = accuracy_score(y_data, y_pred)\n",
    "#                     report = classification_report(y_data, y_pred, output_dict=True, zero_division=0)\n",
    "#                     cm = confusion_matrix(y_data, y_pred)\n",
    "#                     if y_proba is not None:\n",
    "#                         fpr, tpr, _ = roc_curve(y_data, y_proba)\n",
    "#                         roc_auc = auc(fpr, tpr)\n",
    "#                     else:\n",
    "#                         roc_auc = None\n",
    "                \n",
    "#                 self.results[data_type][name] = {\n",
    "#                     'accuracy': acc,\n",
    "#                     'report': report,\n",
    "#                     'confusion_matrix': cm,\n",
    "#                     'roc_auc': roc_auc,\n",
    "#                     'fpr': fpr if 'fpr' in locals() else None,\n",
    "#                     'tpr': tpr if 'tpr' in locals() else None,\n",
    "#                     'y_true': y_data,\n",
    "#                     'y_pred': y_pred\n",
    "#                 }\n",
    "#             except Exception as e:\n",
    "#                 error_msg = f\"Error evaluating {name} on {data_type} data: {str(e)}\"\n",
    "#                 self._error_messages.append(error_msg)\n",
    "#                 print(error_msg)\n",
    "\n",
    "#     def plot_eeg_comparison(self, healthy_data, patient_data, healthy_sfreq=250, patient_sfreq=100, samples=500, channels=3):\n",
    "#         \"\"\"Horizontal EEG signal comparison plot with side-by-side channels\"\"\"\n",
    "#         plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        \n",
    "#         # Create figure with subplots for each channel\n",
    "#         fig, axes = plt.subplots(channels, 2, figsize=(20, channels*3), sharey=True)\n",
    "        \n",
    "#         # Time vectors\n",
    "#         time_h = np.arange(min(samples, len(healthy_data))) / healthy_sfreq\n",
    "#         time_p = np.arange(min(samples, len(patient_data))) / patient_sfreq\n",
    "        \n",
    "#         # Plot each channel horizontally\n",
    "#         for ch in range(min(channels, healthy_data.shape[1], patient_data.shape[1])):\n",
    "#             # Healthy EEG\n",
    "#             axes[ch, 0].plot(time_h, healthy_data[:len(time_h), ch] * 1e6,\n",
    "#                             linewidth=1.5, alpha=0.8, color='blue')\n",
    "#             axes[ch, 0].set_title(f'Healthy - Channel {ch+1}', fontsize=12)\n",
    "#             axes[ch, 0].grid(True, linestyle='--', alpha=0.6)\n",
    "#             axes[ch, 0].set_ylim(-100, 100)\n",
    "            \n",
    "#             # Patient EEG\n",
    "#             axes[ch, 1].plot(time_p, patient_data[:len(time_p), ch] * 1e6,\n",
    "#                             linewidth=1.5, alpha=0.8, color='red')\n",
    "#             axes[ch, 1].set_title(f'Patient - Channel {ch+1}', fontsize=12)\n",
    "#             axes[ch, 1].grid(True, linestyle='--', alpha=0.6)\n",
    "#             axes[ch, 1].set_ylim(-100, 100)\n",
    "        \n",
    "#         # Set common labels\n",
    "#         for ax in axes[:, 0]:\n",
    "#             ax.set_ylabel('Amplitude (μV)', fontsize=12)\n",
    "        \n",
    "#         for ax in axes[-1, :]:\n",
    "#             ax.set_xlabel('Time (seconds)', fontsize=12)\n",
    "        \n",
    "#         plt.suptitle('EEG Signal Comparison (First 500 Samples)', fontsize=16, y=1.02)\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "#     def plot_patient_response_categories(self):\n",
    "#         \"\"\"Enhanced patient response categorization visualization with robust error handling\"\"\"\n",
    "#         if not self.results or 'Patient' not in self.results:\n",
    "#             print(\"No patient results available for visualization\")\n",
    "#             return\n",
    "#         try:\n",
    "#             # Use the first available model's results\n",
    "#             model_name = next(iter(self.results['Patient']))\n",
    "#             results = self.results['Patient'][model_name]\n",
    "#             if 'y_true' not in results or 'y_pred' not in results:\n",
    "#                 print(\"Missing required data in results\")\n",
    "#                 return\n",
    "#             y_true = results['y_true']\n",
    "#             y_pred = results['y_pred']\n",
    "#             # Calculate accuracy for each sample\n",
    "#             if len(np.unique(y_true)) == 2:\n",
    "#                 accuracies = (y_pred == y_true).astype(float)\n",
    "#             else:\n",
    "#                 accuracies = np.array([1.0 if pred == true else 0.0 \n",
    "#                                      for pred, true in zip(y_pred, y_true)])\n",
    "#             # Categorize patients with safe division\n",
    "#             categories = []\n",
    "#             for acc in accuracies:\n",
    "#                 if acc >= 0.7:\n",
    "#                     categories.append(\"Good Response\")\n",
    "#                 elif 0.4 <= acc < 0.7:\n",
    "#                     categories.append(\"Medium Response\")\n",
    "#                 else:\n",
    "#                     categories.append(\"Poor Response\")\n",
    "#             category_counts = pd.Series(categories).value_counts()\n",
    "#             # Create figure with proper layout\n",
    "#             fig = plt.figure(figsize=(18, 8))\n",
    "#             gs = GridSpec(1, 2, width_ratios=[1, 1.5])\n",
    "#             # Subplot 1: Pie chart with safe explode parameter\n",
    "#             ax1 = fig.add_subplot(gs[0])\n",
    "#             colors = ['#4CAF50', '#FFC107', '#F44336']\n",
    "#             # Ensure explode matches the number of categories\n",
    "#             explode = (0.05, 0.05, 0.05)[:len(category_counts)]\n",
    "#             # Handle case where we might have fewer than 3 categories\n",
    "#             if len(category_counts) < 3:\n",
    "#                 colors = colors[:len(category_counts)]\n",
    "#                 explode = explode[:len(category_counts)]\n",
    "#             wedges, texts, autotexts = ax1.pie(\n",
    "#                 category_counts, \n",
    "#                 labels=category_counts.index, \n",
    "#                 autopct=lambda p: f'{p:.1f}%' if p > 0 else '',\n",
    "#                 startangle=90, \n",
    "#                 colors=colors,\n",
    "#                 explode=explode,\n",
    "#                 textprops={'fontsize': 12}\n",
    "#             )\n",
    "#             for autotext in autotexts:\n",
    "#                 autotext.set_color('white')\n",
    "#                 autotext.set_fontweight('bold')\n",
    "#             ax1.set_title('Patient Response Distribution', fontsize=16, pad=20)\n",
    "#             # Subplot 2: Bar plot\n",
    "#             ax2 = fig.add_subplot(gs[1])\n",
    "#             barplot = sns.barplot(\n",
    "#                 x=category_counts.index, \n",
    "#                 y=category_counts.values, \n",
    "#                 ax=ax2,\n",
    "#                 palette=colors,\n",
    "#                 saturation=0.8\n",
    "#             )\n",
    "#             ax2.set_title('Patient Response Categories', fontsize=16, pad=20)\n",
    "#             ax2.set_xlabel('Response Category', fontsize=14)\n",
    "#             ax2.set_ylabel('Number of Patients', fontsize=14)\n",
    "#             # Add count annotations\n",
    "#             for p in barplot.patches:\n",
    "#                 height = p.get_height()\n",
    "#                 if not np.isnan(height) and height > 0:\n",
    "#                     barplot.annotate(\n",
    "#                         f'{int(height)}',\n",
    "#                         (p.get_x() + p.get_width() / 2., height),\n",
    "#                         ha='center', va='center',\n",
    "#                         xytext=(0, 10),\n",
    "#                         textcoords='offset points',\n",
    "#                         fontsize=12,\n",
    "#                         fontweight='bold'\n",
    "#                     )\n",
    "#             # Add overall accuracy if available\n",
    "#             if 'accuracy' in results:\n",
    "#                 overall_acc = results['accuracy']\n",
    "#                 fig.text(\n",
    "#                     0.5, -0.05,\n",
    "#                     f'Model: {model_name} | Overall Accuracy: {overall_acc:.1%}',\n",
    "#                     ha='center', va='center', fontsize=14\n",
    "#                 )\n",
    "#             plt.suptitle('Patient Response Categorization', fontsize=18, y=1.05)\n",
    "#             plt.tight_layout()\n",
    "#             plt.show()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error generating patient response visualization: {str(e)}\")\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "\n",
    "#     def plot_confusion_matrices(self, data_type='Patient'):\n",
    "#         \"\"\"Enhanced confusion matrix visualization with better formatting\"\"\"\n",
    "#         if not self.results or data_type not in self.results:\n",
    "#             print(f\"No results available for {data_type} data\")\n",
    "#             return\n",
    "#         try:\n",
    "#             models = list(self.results[data_type].keys())\n",
    "#             num_models = len(models)\n",
    "#             if num_models == 0:\n",
    "#                 print(\"No models available for visualization\")\n",
    "#                 return\n",
    "#             # Create figure with appropriate size\n",
    "#             fig, axes = plt.subplots(1, num_models, figsize=(6*num_models, 5))\n",
    "#             if num_models == 1:\n",
    "#                 axes = [axes]\n",
    "#             for i, model_name in enumerate(models):\n",
    "#                 model_results = self.results[data_type][model_name]\n",
    "#                 if 'confusion_matrix' not in model_results:\n",
    "#                     print(f\"No confusion matrix for {model_name}\")\n",
    "#                     continue\n",
    "#                 cm = model_results['confusion_matrix']\n",
    "#                 # Get class names\n",
    "#                 if self.label_encoder:\n",
    "#                     classes = self.label_encoder.classes_\n",
    "#                 else:\n",
    "#                     # Handle binary and multiclass cases\n",
    "#                     n_classes = cm.shape[0]\n",
    "#                     classes = [f'Class {i}' for i in range(n_classes)]\n",
    "#                     if n_classes == 2:\n",
    "#                         classes = ['Negative', 'Positive']\n",
    "#                 # Normalize the confusion matrix\n",
    "#                 cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#                 # Plot with annotations\n",
    "#                 sns.heatmap(\n",
    "#                     cm_normalized, \n",
    "#                     annot=True, \n",
    "#                     fmt='.2f',\n",
    "#                     cmap='Blues',\n",
    "#                     xticklabels=classes,\n",
    "#                     yticklabels=classes,\n",
    "#                     ax=axes[i],\n",
    "#                     cbar=False,\n",
    "#                     annot_kws={'fontsize': 10},\n",
    "#                     vmin=0, vmax=1\n",
    "#                 )\n",
    "#                 axes[i].set_title(f'{model_name} Confusion Matrix', fontsize=14)\n",
    "#                 axes[i].set_xlabel('Predicted Label', fontsize=12)\n",
    "#                 axes[i].set_ylabel('True Label', fontsize=12)\n",
    "#             plt.suptitle(f'Model Performance on {data_type} Data', fontsize=16, y=1.05)\n",
    "#             plt.tight_layout()\n",
    "#             plt.show()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error generating confusion matrices: {str(e)}\")\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "\n",
    "#     def plot_roc_curves_comparison(self):\n",
    "#         \"\"\"Plot ROC curves for all models for comparison\"\"\"\n",
    "#         if not self.results or 'Patient' not in self.results:\n",
    "#             print(\"No patient results available\")\n",
    "#             return\n",
    "#         plt.figure(figsize=(10, 8))\n",
    "#         for model_name, results in self.results['Patient'].items():\n",
    "#             if results.get('roc_auc') is not None:\n",
    "#                 plt.plot(results['fpr'], results['tpr'],\n",
    "#                         label=f'{model_name} (AUC = {results[\"roc_auc\"]:.2f})')\n",
    "#         plt.plot([0, 1], [0, 1], 'k--')\n",
    "#         plt.xlim([0.0, 1.0])\n",
    "#         plt.ylim([0.0, 1.05])\n",
    "#         plt.xlabel('False Positive Rate')\n",
    "#         plt.ylabel('True Positive Rate')\n",
    "#         plt.title('Receiver Operating Characteristic Comparison')\n",
    "#         plt.legend(loc=\"lower right\")\n",
    "#         plt.grid(True)\n",
    "#         plt.show()\n",
    "\n",
    "#     def plot_model_performance_comparison(self):\n",
    "#         \"\"\"Enhanced model performance comparison visualization\"\"\"\n",
    "#         if not self.results or 'Patient' not in self.results:\n",
    "#             raise ValueError(\"Patient results not available\")\n",
    "#         models = list(self.results['Patient'].keys())\n",
    "#         metrics = ['accuracy', 'precision', 'recall', 'f1-score']\n",
    "#         metrics_data = []\n",
    "#         for model in models:\n",
    "#             report = self.results['Patient'][model]['report']\n",
    "#             if isinstance(report, dict) and 'accuracy' in report:\n",
    "#                 if 'macro avg' in report:\n",
    "#                     metrics_data.append({\n",
    "#                         'Model': model,\n",
    "#                         'Accuracy': report['accuracy'],\n",
    "#                         'Precision': report['macro avg']['precision'],\n",
    "#                         'Recall': report['macro avg']['recall'],\n",
    "#                         'F1-Score': report['macro avg']['f1-score']\n",
    "#                     })\n",
    "#                 elif len(report.keys()) > 3:\n",
    "#                     metrics_data.append({\n",
    "#                         'Model': model,\n",
    "#                         'Accuracy': report['accuracy'],\n",
    "#                         'Precision': report['1']['precision'],\n",
    "#                         'Recall': report['1']['recall'],\n",
    "#                         'F1-Score': report['1']['f1-score']\n",
    "#                     })\n",
    "#         if not metrics_data:\n",
    "#             raise ValueError(\"No valid metric data found\")\n",
    "#         df = pd.DataFrame(metrics_data)\n",
    "#         df_melted = df.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
    "#         plt.figure(figsize=(12, 6))\n",
    "#         barplot = sns.barplot(\n",
    "#             x='Model', \n",
    "#             y='Score', \n",
    "#             hue='Metric', \n",
    "#             data=df_melted,\n",
    "#             palette='viridis',\n",
    "#             alpha=0.8\n",
    "#         )\n",
    "#         plt.title('Model Performance Metrics Comparison', fontsize=16, pad=20)\n",
    "#         plt.xlabel('Model', fontsize=14)\n",
    "#         plt.ylabel('Score', fontsize=14)\n",
    "#         plt.ylim(0, 1.1)\n",
    "#         plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "#         for p in barplot.patches:\n",
    "#             barplot.annotate(\n",
    "#                 format(p.get_height(), '.2f'),\n",
    "#                 (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                 ha='center', va='center',\n",
    "#                 xytext=(0, 10),\n",
    "#                 textcoords='offset points',\n",
    "#                 fontsize=10\n",
    "#             )\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "#     def run_pipeline(self, config):\n",
    "#         \"\"\"Complete EEG analysis pipeline with robust error handling\"\"\"\n",
    "#         results = None\n",
    "#         try:\n",
    "#             print(\"\\n=== EEG Analysis Pipeline ===\\n\")\n",
    "            \n",
    "#             # 1. Load data\n",
    "#             print(\"[1/7] Loading data...\")\n",
    "#             healthy_data, healthy_labels, h_sfreq = self.load_dataset(\n",
    "#                 config['healthy_path'],\n",
    "#                 max_files=config.get('max_files'),\n",
    "#                 max_duration=config.get('max_duration', None))\n",
    "            \n",
    "#             patient_data, patient_labels, p_sfreq = self.load_dataset(\n",
    "#                 config['patient_path'],\n",
    "#                 is_patient=True,\n",
    "#                 max_files=config.get('max_files'),\n",
    "#                 max_duration=config.get('max_duration', None))\n",
    "            \n",
    "#             # Verify data was loaded\n",
    "#             if healthy_data is None or len(healthy_data) == 0:\n",
    "#                 raise ValueError(\"No healthy data loaded - check file formats and paths\")\n",
    "#             if patient_data is None or len(patient_data) == 0:\n",
    "#                 raise ValueError(\"No patient data loaded - check file formats and paths\")\n",
    "            \n",
    "#             # 2. Channel matching\n",
    "#             print(\"[2/7] Finding common channels...\")\n",
    "#             if not config.get('skip_channel_matching', False):\n",
    "#                 healthy_channels = self.collect_channel_names(config['healthy_path'])\n",
    "#                 patient_channels = self.collect_channel_names(config['patient_path'], is_patient=True)\n",
    "#                 self.common_channels = self.find_common_channels(healthy_channels, patient_channels)\n",
    "            \n",
    "#             # 3. Preprocessing\n",
    "#             print(\"[3/7] Preprocessing data...\")\n",
    "#             X_healthy, y_healthy = self.preprocess_data(\n",
    "#                 healthy_data, healthy_labels,\n",
    "#                 sfreq=h_sfreq,\n",
    "#                 common_channels=self.common_channels)\n",
    "            \n",
    "#             X_patients, y_patients = self.preprocess_data(\n",
    "#                 patient_data, patient_labels,\n",
    "#                 sfreq=p_sfreq,\n",
    "#                 common_channels=self.common_channels)\n",
    "            \n",
    "#             # Verify preprocessing succeeded\n",
    "#             if X_healthy is None or X_patients is None:\n",
    "#                 raise ValueError(\"Preprocessing failed - check data compatibility\")\n",
    "            \n",
    "#             # 4. Load artifacts\n",
    "#             print(\"[4/7] Loading preprocessing artifacts...\")\n",
    "#             self.load_preprocessing_artifacts(\n",
    "#                 config['scaler_path'],\n",
    "#                 config['label_encoder_path'])\n",
    "            \n",
    "#             # 5. Apply transformations\n",
    "#             print(\"[5/7] Applying transformations...\")\n",
    "#             if self.scaler:\n",
    "#                 expected_features = self.scaler.n_features_in_\n",
    "#                 X_healthy = self.match_features(X_healthy, expected_features)\n",
    "#                 X_patients = self.match_features(X_patients, expected_features)\n",
    "#                 X_healthy = self.scaler.transform(X_healthy)\n",
    "#                 X_patients = self.scaler.transform(X_patients)\n",
    "            \n",
    "#             if self.label_encoder is not None and y_patients is not None:\n",
    "#                 y_patients = self.label_encoder.transform(y_patients)\n",
    "            \n",
    "#             # 6. Load models\n",
    "#             print(\"[6/7] Loading models...\")\n",
    "#             self.load_models(config['model_paths'])\n",
    "            \n",
    "#             # 7. Evaluate models (only on patient data)\n",
    "#             print(\"[7/7] Evaluating models...\")\n",
    "#             self.evaluate_models(X_patients, y_patients, data_type='Patient')\n",
    "            \n",
    "#             # Generate visualizations\n",
    "#             print(\"\\n=== Generating Visualizations ===\\n\")\n",
    "#             try:\n",
    "#                 plt.style.use('seaborn-v0_8-whitegrid')\n",
    "#             except:\n",
    "#                 plt.style.use('ggplot')\n",
    "                \n",
    "#             vis_funcs = [\n",
    "#                 ('EEG Comparison', self.plot_eeg_comparison, [healthy_data, patient_data, h_sfreq, p_sfreq]),\n",
    "#                 ('Patient Response', self.plot_patient_response_categories, []),\n",
    "#                 ('Confusion Matrix', self.plot_confusion_matrices, ['Patient']),\n",
    "#                 ('Model Performance', self.plot_model_performance_comparison, []),\n",
    "#                 ('ROC Curve', self.plot_roc_curves_comparison, [])\n",
    "#             ]\n",
    "            \n",
    "#             for name, func, args in vis_funcs:\n",
    "#                 try:\n",
    "#                     print(f\"Generating {name} visualization...\")\n",
    "#                     func(*args)\n",
    "#                     plt.close('all')\n",
    "#                 except Exception as e:\n",
    "#                     error_msg = f\"Failed to generate {name}: {str(e)}\"\n",
    "#                     self._error_messages.append(error_msg)\n",
    "#                     print(error_msg)\n",
    "            \n",
    "#             print(\"\\n=== Pipeline Completed ===\\n\")\n",
    "#             if self._error_messages:\n",
    "#                 print(\"Completed with warnings/errors (see above messages)\")\n",
    "#             else:\n",
    "#                 print(\"Completed successfully!\")\n",
    "                \n",
    "#             results = {\n",
    "#                 'healthy_data': (X_healthy, y_healthy),\n",
    "#                 'patient_data': (X_patients, y_patients),\n",
    "#                 'results': self.results,\n",
    "#                 'metadata': {\n",
    "#                     'healthy_samples': len(X_healthy),\n",
    "#                     'patient_samples': len(X_patients),\n",
    "#                     'common_channels': self.common_channels,\n",
    "#                     'features_per_channel': self.expected_features_per_channel,\n",
    "#                     'errors': self._error_messages\n",
    "#                 }\n",
    "#             }\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"\\n!!! Pipeline Failed !!!\\nError: {str(e)}\")\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "            \n",
    "#         finally:\n",
    "#             if 'healthy_data' in locals():\n",
    "#                 del healthy_data\n",
    "#             if 'patient_data' in locals():\n",
    "#                 del patient_data\n",
    "#             gc.collect()\n",
    "            \n",
    "#         return results\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     channel_mapping = {\n",
    "#         'FP1': 'Fp1',\n",
    "#         'FP2': 'Fp2',\n",
    "#         'F3': 'F3',\n",
    "#         'F4': 'F4',\n",
    "#     }\n",
    "#     config = {\n",
    "#         'healthy_path': \"F:/shivani/VSCode/ml/worked on dataset/3(final)/Healthy\",\n",
    "#         'patient_path': \"F:/shivani/VSCode/ml/worked on dataset/4/dataset/Patients\",\n",
    "#         'model_paths': {\n",
    "#             \"SVM\": \"F:/shivani/VSCode/ml/worked on dataset/3(final)/svm_model.pkl\",\n",
    "#             \"RF\": \"F:/shivani/VSCode/ml/worked on dataset/3(final)/rf_model.pkl\",\n",
    "#             \"XGB\": \"F:/shivani/VSCode/ml/worked on dataset/3(final)/xgb_model.pkl\",\n",
    "#             \"LSTM\": \"F:/shivani/VSCode/ml/worked on dataset/3(final)/lstm_model.h5\",\n",
    "#             \"CNN\": \"F:/shivani/VSCode/ml/worked on dataset/3(final)/cnn_model.h5\"\n",
    "#         },\n",
    "#         'scaler_path': \"F:/shivani/VSCode/ml/worked on dataset/3(final)/preprocessing_artifacts/eeg_scaler.joblib\",\n",
    "#         'label_encoder_path': \"F:/shivani/VSCode/ml/worked on dataset/3(final)/preprocessing_artifacts/label_encoder.joblib\",\n",
    "#         'max_files': 8,\n",
    "#     }\n",
    "#     pipeline = EEGPipeline()\n",
    "#     pipeline.set_channel_mapping(channel_mapping)\n",
    "#     results = pipeline.run_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_names_from_mat(mat_path):\n",
    "    try:\n",
    "        mat_data = loadmat(mat_path)\n",
    "        if 'data' in mat_data:\n",
    "            data_struct = mat_data['data'][0][0]\n",
    "            if 'channels' in data_struct.dtype.names:\n",
    "                return [ch[0] for ch in data_struct['channels'][0]]  # Convert array of arrays to list\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {mat_path}: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_names_from_vhdr(vhdr_path):\n",
    "    try:\n",
    "        raw = mne.io.read_raw_brainvision(vhdr_path, preload=False, verbose=False)\n",
    "        return raw.ch_names\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {vhdr_path}: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "def inspect_mat_file(mat_path):\n",
    "    \"\"\"Inspect the structure of a .mat file.\"\"\"\n",
    "    try:\n",
    "        mat_data = loadmat(mat_path)\n",
    "        print(f\"Keys in {mat_path}: {list(mat_data.keys())}\")\n",
    "        for key in mat_data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                print(f\"{key}: {type(mat_data[key])}, shape: {mat_data[key].shape if hasattr(mat_data[key], 'shape') else 'N/A'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {mat_path}: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "inspect_mat_file(\"F:/shivani/VSCode/ml/worked on dataset/4/dataset/Patients/A01.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_channels(healthy_channels, patient_channels):\n",
    "    \"\"\"Find common channels between two datasets.\"\"\"\n",
    "    healthy_set = set(healthy_channels)\n",
    "    patient_set = set(patient_channels)\n",
    "    common_channels = healthy_set.intersection(patient_set)\n",
    "    return sorted(common_channels)  # Return sorted list for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_channel_names(folder_path, is_patient=False):\n",
    "    \"\"\"Collect all unique channel names from a folder of files.\"\"\"\n",
    "    all_channels = set()\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.vhdr' if not is_patient else '.mat')]\n",
    "    for file in tqdm(files, desc=f\"Collecting {'patient' if is_patient else 'healthy'} channels\"):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        if is_patient:\n",
    "            channels = get_channel_names_from_mat(file_path)\n",
    "        else:\n",
    "            channels = get_channel_names_from_vhdr(file_path)\n",
    "        all_channels.update(channels)\n",
    "    return sorted(all_channels)  # Return sorted list for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_channels = collect_channel_names(config['healthy_path'])\n",
    "patient_channels = collect_channel_names(config['patient_path'], is_patient=True)\n",
    "\n",
    "# Find common channels\n",
    "common_channels = find_common_channels(healthy_channels, patient_channels)\n",
    "print(f\"Common channels: {common_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
